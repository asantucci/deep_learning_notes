\documentclass[12pt]{article}  
\usepackage[boxruled,lined]{algorithm2e} 
\usepackage{booktabs}
\usepackage{amsmath} 
\usepackage{amsthm} 
\usepackage{amsfonts} 
\usepackage{enumitem}
\usepackage{xparse} 
\usepackage{bbm} 
\usepackage{color,soul} 
\usepackage{framed} \usepackage[margin=0.5in]{geometry} \usepackage{hyperref} \usepackage{mathtools}
\usepackage{multicol}
\usepackage[dvipsnames]{xcolor}
\usepackage{tikz}
\usepackage[normalem]{ulem}
\usepackage{pgfplots}  
\usetikzlibrary{positioning}
\usetikzlibrary{calc}
\usetikzlibrary{fit}
\usetikzlibrary{shapes}
\usetikzlibrary{patterns}

\newtheorem{theorem}{Theorem}[section] \newtheorem{lemma}[theorem]{Lemma} \newtheorem{proposition}[theorem]{Proposition} \newtheorem{corollary}[theorem]{Corollary}  \DeclarePairedDelimiter{\ceil}{\lceil}{\rceil} \DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor} \DeclareMathOperator*{\argmin}{arg\,min} \DeclareMathOperator*{\argmax}{arg\,max} \newcommand{\D}{\mathrm{d}} \SetKwInput{KwInput}{Input} \SetKwInput{KwOutput}{Output}  \begin{document}
\renewcommand{\d}[1]{\ensuremath{\operatorname{d}\!{#1}}}

\section{Logistic Regression}\vspace{.1pt} \hrule height 2pt \smallskip \renewcommand{\arraystretch}{1}% Tighter \begin{description} \item[Intro] 
This model assumes that the log odds-ratio can be fit by a linear model. We start with the probability mass function for a \href{https://en.wikipedia.org/wiki/Bernoulli_distribution}{Bernoulli trial}: $f(y;p) = p^y \times (1-p)^{(1-y)}$ for $y \in \{0, 1\}$. We are interested in estimating what $p$ is, and so it's natural to conceive of a maximum likelihood estimator; since $\log$ monotone we can apply this transformation without changing the maximizer. The log-likelihood is given by $y \log p + (1-y) \log (1-p)$ using \href{https://en.wikipedia.org/wiki/Logarithm#Logarithmic_identities}{properties of logarithms}. In optimization, we generally work to minimize objective functions, and so it's then natural to set our (objective) \emph{loss function}
to be $- \left[y \log p + (1-y) \log (1-p)\right]$. In summary, we 
have, for $\sigma(x) = \frac{1}{1 + \exp(-x)}$:
\begin{align}
  z &= w^Tx + b \\
  \hat y &= a = \sigma(z) \\
  \mathcal{L}(a, y) &= - \left[y \log (a) + (1-y) \log (1-a)\right] \end{align}

We can draw a \emph{computation graph} to describe the forward pass as follows:
\begin{center}
\begin{tikzpicture}
  \node (x) at (0, 1) {x};
  \node (w) at (0,.5) {w};
  \node (b) at (0, 0) {b};
  \draw[->] (x) to (1, .6);
  \draw[->] (w) to (1, .5);
  \draw[->] (b) to (1, .4);
  \node[draw=black,rectangle] (z) at (2.5, .5) {$z = w^Tx + b$};
  \draw[->] (z) to (4.25, .5);
  \node[draw=black,rectangle] (yhat) at (5.5, .5) {$\hat y = a = \sigma(z)$};
  \draw[->] (yhat) to (7.25, .5);
  \node[draw=black,rectangle] (loss) at (8, .5) {$\mathcal L (a,y)$};
\end{tikzpicture}
\end{center} We seek to learn $w, b$ to minimize the loss function. \emph{Back propogation} proceeds as follows:

{
\small{
\begin{align*}
\texttt{da} &= \frac{\partial \mathcal L}{\partial a} = - \left(\frac{y}{a} - \frac{1-y}{1-a}\right) = \frac{-y}{a} + \frac{1-y}{1-a}. \\
  \texttt{dz} &= \frac{\partial \mathcal L}{\partial z} = \frac{\partial \mathcal L}{\partial a}\frac{\partial a}{\partial z} = \left(\frac{-y}{a} + \frac{1-y}{1-a}\right) \times a(1-a) = \frac{-y}{a} \cdot a(1-a) + \frac{1-y}{1-a} \cdot (1-a) a = -y(1-a)+ (1-y)a \\ &= ay - y + a - ay = a - y.  \\
\texttt{dw} &= \frac{\partial \mathcal L}{\partial w} \overset{?}{=} \frac{\partial \mathcal L}{\partial z} \frac{\partial z}{\partial w} = (a-y) x^T. \\
\texttt{db} &= \frac{\partial \mathcal L}{\partial b} = \frac{\partial \mathcal L}{\partial z} \underbrace{\frac{\partial z}{\partial b}}_{=1}. \end{align*}
}
}
Our update rule then becomes: $w := w - \alpha \texttt{dw}$, and $b := b - \alpha \texttt{db}$. Our (average) \emph{cost} function is defined as $J(w,b) = \frac{1}{m} \sum_{i=1}^m \mathcal L(a^{(i)}, y^{(i)})$. Since $\frac{\partial}{\partial \cdot}$ is a linear operator, obtaining gradients is quite straightforward since we are left with a series of derivatives of loss functions, which we calculated above.
\begin{align*}    \frac{\partial J}{\partial w} = \frac{1}{m} \sum \frac{\partial \mathcal L}{\partial w}, \hspace{15pt} \textrm{and} \hspace{15pt} \frac{\partial J}{\partial b} = \frac{1}{m} \sum \frac{\partial \mathcal L}{\partial b} \end{align*}
Our optimization routine then can be written as in algorithm \ref{alg: logisticreg}

{\small
\begin{algorithm}[H]
  \label{alg: logisticreg}
  \caption{Logistic Regression - Optimization}   \For{\texttt{i in range(m)}} {
    $z^{(i)} = w^Tx^{(i)} + b$ \\
    $a^{(i)} = \sigma(z^{(i)})$ \\
    $J += - \left[ y^{(i)} \log a^{(i)} + (1-y^{(i)}) \log (1 - a^{(i)}) \right]$ \\
    $\partial d z^{(i)} = a^{(i)} - y^{(i)}$ \\
    $\partial d w += \partial d z^{(i)} {x^{(i)}}^T$ \\
    $\partial d b += \partial d z^{(i)}$
}
$J /= m$ \\
$\partial w /= m$ \\
$\partial b /= m$ \end{algorithm}
}
The above concludes one round of \href{https://en.wikipedia.org/wiki/Gradient_descent}{Gradient Descent}. We repeat this procedure many times until training loss (and ideally test loss ) is sufficiently minimized. We remark that it's possible to remove both \texttt{for} loops (over the training data, and over the parameters in $w$) by using vectorized operations in \texttt{numpy}. We execute this in code \href{https://github.com/asantucci/NN/blob/master/logistic_regression.py#L46}{here}.

\section{Neural Networks with 2-layers}
\vspace{-10pt}
We previously saw a simple computation graph. 
\begin{tikzpicture}   \node (w) at (0, 1) {w};
  \node (x) at (0, .5) {x};
  \node (b) at (0,0) {b};
  \draw[->] (x) to (1, .5);
  \draw[->] (w) to (1, .6);
  \draw[->] (b) to (1, .4);
  \node[draw=black,rectangle] (z) at (2.5, .5) {$z = w^Tx + b$};
  \draw[->] (z) to (4.25, .5);
  \node[draw=black,rectangle] (yhat) at (5.5, .5) {$\hat y = a = \sigma(z)$};
  \draw[->] (yhat) to (7.25, .5);
  \node[draw=black,rectangle] (loss) at (8, .5) {$\mathcal L (a,y)$};
\end{tikzpicture}
\subsection{Understanding Small Neural Networks}
A Neural Network can be constructed by stacking together sigmoids, depicted as follows:
\begin{figure}[h]
\centering
\begin{tikzpicture}
  \node (x1) at (0, 1) {$x_1$};
  \node (x2) at (0, 0) {$x_2$};
  \node (x3) at (0,-1) {$x_3$};
  \node (inputlayer) at (0, -3) {$\substack{\textrm{input}\\ \textrm{layer}}$};
  \draw[->,dashed] (inputlayer) to (x3);
  \node (lbl) at (1,2) {\small{$[1]$}}; 
  \node[draw=black,circle,inner sep = 0pt] (n1) at (1, 1) {\tiny $a_1^{[1]}$};
  \draw[->,dashed] (lbl) to (n1);
  \node[draw=black,circle,inner sep = 0pt] (n2) at (1, 0) {\tiny $a_2^{[1]}$};
  \node[draw=black,circle,inner sep = 0pt] (n3) at (1,-1) {\tiny $a_3^{[1]}$};
  \node (hiddenlayer) at (1, -3) {$\substack{\textrm{hidden}\\ \textrm{layer}}$};
  \draw[->,dashed] (hiddenlayer) to (n3);
  \draw[->] (x1) to (n1);
  \draw[->] (x1) to (n2);
  \draw[->] (x1) to (n3);   \draw[->] (x2) to (n1);
  \draw[->] (x2) to (n2);
  \draw[->] (x2) to (n3);   
  \draw[->] (x3) to (n1);
  \draw[->] (x3) to (n2);
  \draw[->] (x3) to (n3); 
  \node[draw=black,circle] (m1) at (2,0) {};
  \node (lbl2) at (2,2) {\small{$[2]$}}; 
  \draw[->,dashed] (lbl2) to (m1);
  \node (yhat) at (3,0) {$\hat y = a^{[2]}$};
  \node (outputlayer) at (2, -3) {$\substack{\textrm{output}  \\ \textrm{layer}}$};
  \draw[->,dashed] (outputlayer) to (m1);
  \draw[->] (n1) to (m1);
  \draw[->] (n2) to (m1);
  \draw[->] (n3) to (m1);
  \draw[->] (m1) to (yhat);
  \node (x) at (2.5, -1) {$x$};
  \node (w1) at (2.5, -1.5) {$W^{[1]}$};
  \node (b1) at (2.5, -2) {$b^{[1]}$};
  \node[draw=black,rectangle] (z1) at (5.25, -1.5) {$z^{[1]} = W^{[1]}x + b^{[1]}$};
  \draw[->] (x) to (z1);
  \draw[->] (w1) to (z1);
  \draw[->] (b1) to (z1);
  \node[draw=black,rectangle] (a1) at (8.5, -1.5) {$a^{[1]} = \sigma (z^{[1]})$};
  \draw[->] (z1) to (a1);
  \node[draw=black,rectangle] (z2) at (12.25, -1.5) {$z^{[2]} = W^{[2]}x + a^{[1]} + b^{[2]}$};
  \draw[->] (a1) to (z2);
  \node (w2) at (9.2, -2.25) {$W^{[2]}$};
  \node (b2) at (9.2, -2.75) {$b^{[2]}$};
  \draw[->] (w2) to (9.9, -1.5);
  \draw[->] (b2) to (9.9, -1.5);
  \node[draw=black,rectangle] (a2) at (16, -1.5) {$a^{[2]} = \sigma(z^{[2]})$};
  \draw[->] (z2) to (a2);
  \node[draw=black,rectangle] (l) at (18.75, -1.5) {$\mathcal L(a^{[2]},y)$};
  \draw[->] (a2) to (l);
\end{tikzpicture}
\caption{A 2-layer Neural Network (you could say we don't count the input layer, or you could say we do but we index starting from zero).}
\end{figure}
\newline
\vspace{-40pt}
\paragraph{Terminology and Notation}
Each \emph{neuron} in the graph consists of both a linear transformation and a non-linear activation function, i.e. the first stack of nodes will produce a $z$ and an $a$. We use the super-script square brackets $[\hspace{2pt} ]$ to denote a stack of nodes, i.e. a layer, not to be confused with super-script parentheses which index training examples. I.e. $a^{[\ell]}_i$ denotes the output of an activation function in layer $\ell$ for the $i$th neuron.
The key difference between our Logistic Regression and 
this (or any) Neural Network is that we simply repeat linear transformations followed by non-linear activation functions \emph{multiple times}.
The reason why we call the intermediary layer a ``hidden'' layer is because we do not observe what these values are to be in the training set. By convention, we define  $a^{[0]} = X$. We can further refer to the hidden layer by ${a^{[1]}} = \begin{bmatrix}   a_1^{[1]} & a_2^{[1]} & a_3^{[1]}  \end{bmatrix}^T$. Notice that the hidden layer and output layer have parameters $W^{[\cdot]}$ and $b^{[\cdot]}$ associated with them. 

\paragraph{Visualizing a Neuron}
Let's take a look at neural network representation in a bit more detail. We can think about each neuron as being divided into two parts: one which performs a linear transformation and another which performs an activation function. The mental model is:
\begin{figure}[h]
  \centering   
  \begin{tikzpicture}[scale=0.6]     
    \node (x1) at (-1, 1) {$x_1$};
    \node (x2) at (-1, 0) {$x_2$};
    \node (x3) at (-1,-1) {$x_3$};
    \node[draw,circle] (neuron) at (1.5,0) {$\substack{z = w^Tx + b \\ a = \sigma(z)}$};
    \draw[->] (x1) to (neuron);
    \draw[->] (x2) to (neuron);
    \draw[->] (x3) to (neuron);
    \node (output) at (5,0) {$a = \hat y$};
    \draw[->] (neuron) to (output);
  \end{tikzpicture} \end{figure}
\newline
In general, we'll have something as follows for the first neuron in the hidden layer, and to be crystal clear we draw out the second neuron as well.
\begin{tikzpicture}[scale=0.7]     
  \node (x1) at (0, 1) {$x_1$};
    \node (x2) at (0, 0) {$x_2$};
    \node (x3) at (0,-1) {$x_3$};
    \node[draw,circle] (neuron) at (2.5,2) {$\substack{z_1^{[1]} = {w_1^{[1]}}^Tx + b_1^{[1]} \\ a_1^{[1]} = \sigma(z_1^{[1]})}$};
    \draw[->] (x1) to (neuron);
    \draw[->] (x2) to (neuron);
    \draw[->] (x3) to (neuron);
    \node[draw,circle,dashed] (n2) at (2.5,0 ) {};     \node[draw,circle,dashed] (n3) at (2.5,-1) {}; 
    \node[draw,circle,dashed] (n4) at (2.5,-2) {}; 
    \draw[->,dashed] (x1) to (n2);
    \draw[->,dashed] (x2) to (n2);
    \draw[->,dashed] (x3) to (n2);
    \draw[->,dashed] (x1) to (n3);
    \draw[->,dashed] (x2) to (n3);
    \draw[->,dashed] (x3) to (n3);
    \draw[->,dashed] (x1) to (n4);
    \draw[->,dashed] (x2) to (n4);
    \draw[->,dashed] (x3) to (n4);
\end{tikzpicture}
\hspace{20pt} \begin{tikzpicture}[scale=0.7]     
    \node (x1) at (0, 1) {$x_1$};
    \node (x2) at (0, 0) {$x_2$};
    \node (x3) at (0,-1) {$x_3$};
    \node[draw,circle,dashed] (neuron) at (2.75,2) {};
    \draw[->,dashed] (x1) to (neuron);
    \draw[->,dashed] (x2) to (neuron);
    \draw[->,dashed] (x3) to (neuron);
    \node[draw,circle] (n2) at (3.5,0 ) {$\substack{z_2^{[1]} = {w_2^{[1]}}^Tx + b_2^{[1]} \\ a_2^{[1]} = \sigma(z_2^{[1]})}$};     
    \node[draw,circle,dashed] (n3) at (2.75,-2) {}; 
    \node[draw,circle,dashed] (n4) at (2.75,-3) {}; 
    \draw[->] (x1) to (n2);
    \draw[->] (x2) to (n2);
    \draw[->] (x3) to (n2);
    \draw[->,dashed] (x1) to (n3);
    \draw[->,dashed] (x2) to (n3);
    \draw[->,dashed] (x3) to (n3);
    \draw[->,dashed] (x1) to (n4);
    \draw[->,dashed] (x2) to (n4);
    \draw[->,dashed] (x3) to (n4);
\end{tikzpicture} 
To avoid having to calculate $z_i^{[\ell]}$ using a \texttt{for}
loop, we can instead use a matrix multiply, where $W^{[1]} \in \mathbb R^{n_1, n_x}$:
$$
W^{[1]}x + b^{[1]} = 
\underbrace{\begin{bmatrix}    & {w_1^{[1]}}^T & \\
   & {w_2^{[1]}}^T & \\    & {w_3^{[1]}}^T & \\    
   & {w_4^{[1]}}^T & \\ 
\end{bmatrix}}_{\in \mathbb R^{4, 3}} \begin{bmatrix} x_1 \\ x_2 \\ x_3  \end{bmatrix} + \begin{bmatrix}   b_1^{[1]} \\ b_2^{[1]} \\ b_3^{[1]} \\ b_4^{[1]} \end{bmatrix} = \begin{bmatrix}   z_1^{[1]} \\
  z_2^{[1]} \\
  z_3^{[1]} \\
  z_4^{[1]} \\ \end{bmatrix} = z^{[1]}
$$
We can then write ${a^{[1]}}^T = \begin{bmatrix}   a_1^{[1]} & a_2^{[1]} & a_3^{[1]} & a_4^{[1]} \end{bmatrix} = \sigma (z^{[1]})$ where the $\sigma(\cdot)$ is applied element-wise.
Given an input $x$, we set $a^{[0]} = x$ and we compute forward steps: $z^{[\ell]} = W^{[\ell]}a^{[\ell-1]} + b^{[\ell]}$
for each layer in the network.

\paragraph{Vectorization} Suppose we have a single hidden-layer neural network. As per above, the forward propagation involves computing: 
$  z^{[1]} = W^{[1]}a^{[0]} + b^{[1]}; \hspace{10pt} a^{[1]} = \sigma(z^{[1]}); \hspace{10pt} 
   z^{[2]} = W^{[2]}a^{[1]} + b^{[2]}; \hspace{10pt} 
   a^{[2]} = \sigma(z^{[2]}).
$
We need to replicate this procedure for each of our $m$ training samples. I.e. we need to feed each training example through the network to get an output. Let our final output be denoted by $a^{[\ell](j)}$ denote the output for the activation function for the $j$th training example at the $\ell$th layer in our network. So in our case above, $a^{[2](j)}$ is the output for the $j$th training example.
We'd like to avoid applying a \texttt{for} loop over each of our $m$ training examples, like so:
{\footnotesize
\begin{algorithm}[h]
  \caption{Naive Forward Propagation on a 2-layer Neural Network}   \For{i = 1 to m}{
    $z^{[1](i)} = W^{[1]}x^{(i)} + b^{[1]}$ \\
    $a^{[1](i)} = \sigma(z^{[1]}(i))$ \\
    $z^{[2](i)} = W^{[2]}a^{[1](i)} + b^{[2]}$ \\
    $a^{[2](i)} = \sigma(z^{[2]}(i))$
  } \end{algorithm}
}
\newline
\vspace{-.5ex}Recall we arranged our input matrix such that each column is an observation, i.e. $X = \begin{bmatrix}    | & | & & | \\    x^{(1)} & x^{(2)} & \ldots & x^{(m)} \\   | & | & & |  \end{bmatrix} \in \mathbb R^{n_x, m}$. Then,
$  
  Z^{[1]} = W^{[1]} X + b^{[1]}; \hspace{10pt}
  A^{[1]} = \sigma(Z^{[1]}); \hspace{10pt}
  Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}; \hspace{10pt}
  A^{[2]} = \sigma(Z^{[2]})
$ To be explicit, $Z^{[1]}$ is also arranged with observations in columns, i.e. $Z^{[1]} = \begin{bmatrix}   | & | & & | \\ z^{[1](1)} & z^{[1](2)} & \ldots & z^{[1](m)} \\ | & | & & | \end{bmatrix}$ and $A^{[1]} = \begin{bmatrix}    | & | & & | \\    a^{[1](1)} & a^{[1](2)} & \ldots & a^{[1](m)}  \\  | & | & & | \end{bmatrix}$.\footnote{As one scans matrix $A^{[\ell]}$ from left to right, we scan through observations or training examples, and as we scan from top to bottom we scan through the activations of different hidden units.}

\subsection{Activation Functions}
\paragraph{Hyperbolic Tangent}
Another activation function to consider is $\tanh (z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}$. It's a shifted and rescaled version of the sigmoid function, which ranges from $[-1, 1]$ and cross the $x$-axis at zero; it is intuitively thought to perform better than a sigmoid activation function since it has the effect of ``centering'' the data in the intermediate layers of the network around zero. The one exception is the output layer: if we are predicting binary classification it makes sense to use sigmoid since our target values are in $\{0, 1\}$ we therefore want $\hat y$ to be in $(0,1)$.

\begin{figure}[h]
  \centering   
  \begin{tikzpicture}[scale=0.7]   \draw[->] (-3,0) -- (3,0) node[right] {$x$};   \draw[->] (0,-1) -- (0,1) node[above] {$y$};
  \draw[domain=-3:3,smooth,variable=\x,blue] plot ({\x},{(exp(\x) - exp(-\x))/(exp(\x) + exp(-\x))}); 
\end{tikzpicture}   \end{figure}
Both sigmoid and $\tanh$ suffer from the problem of saturating gradients. I.e. if the input value is either very small or very large, the derivative of the function becomes very small. This has the effect of slowing down gradient descent.

\paragraph{Rectified Linear Unit}
This leads us to the rectified linear unit: $a = g(z) = \max\{0, z\}$. The derivative at zero is not well defined, however, it is unlikely that we will ever encounter a true zero in our computations and so we can ignore this case without much worry. One disadvantage of the ReLU is that its derivative is identically zero when the input is negative, but this ends up not harming us in practice: enough neurons will likely have positive inputs such that gradients can still be learned. There is also the leaky ReLU: $a = g(z) = \max\{0.01 \times z, z\}$.

\begin{figure}[h]
  \centering   
  \begin{tikzpicture}[scale=1.2]
    \draw[->] (-1,0) -- (1,0) node[right] {$x$};   
    \draw[->] (0,-1) -- (0,1) node[above] {$y$};
    \draw[domain=-1:1,smooth,variable=\x,blue] plot ({\x},{max(0,\x)}); 
  \end{tikzpicture}   
\end{figure}

\paragraph{Linear Activation Function (Identity Function)} It's worthwhile to realize that if we only used linear activation functions $g(z) = z$, then our neural network would in fact be a linear model. This is easy to see, since if $a^{[1]} = z^{[1]} = W^{[1]} x + b^{[1]}$ and $z^{[2]} = W^{[2]} a^{[1]} + b^{[2]}$ then plugging in for $a^{[1]}$, $z^{[2]} = W^{[2]} (W^{[1]} x + b^{[1]}) + b^{[2]} = \underbrace{W^{[2]}W^{[1]}}_{=W'}x + \underbrace{W^{[2]}b^{[1]} + b^{[2]}}_{b'}$ which is linear with respect to the input $x$. The one time we may use a linear activation function is if, for example, we were predicting housing prices, in which case we may want our output to range arbitrarily high. Even then, a better choice would be ReLU since housing prices are non-negative.

\subsubsection{Derivatives of Activation Functions} 
\paragraph{Hyperbolic Tangent}
Recall that 
\begin{align*}   \sinh(x) = \frac{e^x - e^{-x}}{2}, \hspace{10pt} 
  \cosh(x) = \frac{e^x + e^{-x}}{2}, \hspace{10pt} \textrm{ and } \hspace{5pt}
  \tanh(x) = \frac{\sinh(x)}{\cosh(x)}. \end{align*}
It's easy to see using linearity of $\frac{\d \,}{\d x}$ and the chain rule that $\frac{\d \,}{\d x}  \sinh(x) = \frac{e^x}{2} + \frac{e^{-x}}{2} = \cosh(x)$ and similarly that $\frac{\d \,}{\d x}\cosh(x) = \sinh(x)$. Then, apply either quotient rule \emph{or} both the product rule and chain rule:
\begin{align*}   \frac{\d \,}{\d x} \tanh(x) = \frac{\d \,}{\d x} \sinh(x) \cosh(x)^{-1} = \cosh(x) \cosh(x)^{-1} - \sinh(x)\cosh(x)^{-2}\sinh(x) = 1 - \tanh(x)^2. \end{align*}
\paragraph{Rectified Linear Unit} has derivative equal to
\begin{align}   \frac{\d \,}{\d x} \max\{0,x\}=       \begin{cases}        0 &\quad\text{if } x < 0\\        1 &\quad\text{if } x > 0 \\
       \textrm{undefined} &\quad\text{if } x = 0      \end{cases} \end{align}
Technically the derivative is undefined when $x = 0$, however, in practice a floating point variable never takes on the value zero exactly. We can therefore ignore this case by choosing to ``define'' the gradient to be either zero or one (which we choose again doesn't matter in practice). Either way, note that ReLU is convex and so we realize a \href{https://en.wikipedia.org/wiki/Subderivative}{sub-derivative}.

\paragraph{Leaky ReLU} is defined as $\max \{0.01 x, x \}$ and its derivative is therefore given by
\begin{align}   
  \frac{\d \,}{\d x} \max\{0.01x,x\}=       
  \begin{cases}        
    0.01 &\quad\text{if } x < 0\\        
    1    &\quad\text{if } x > 0 \\
    \textrm{undefined} &\quad\text{if } x = 0      
  \end{cases} 
\end{align}
Again the fact that the derivative doesn't exist at $x=0$ doesn't matter in practice since we're dealing with floating point values that are never identically zero.

\subsection{Back-Propagation for Neural Networks}
\paragraph{Review: Logistic Regression Computation Graph}
Recall our logistic regression computation graph. After computing our forward pass, we needed to compute a backward pass by calculating gradients at each step, depicted in \color{orange}{orange} \color{black} below.
\begin{figure}[h]
\centering \begin{tikzpicture}
  \node (x) at (0, 1) {x};
  \node (w) at (0, .5) {w};
  \node (b) at (0,0) {b};
  \node [left = .1cm of w] {\color{orange}$\d w$}; 
  \node [left = .25cm of b] {\color{orange}$\d b$}; 
  \draw[->] (x) to (1, .6);
  \draw[->] (w) to (1, .5);
  \draw[->] (b) to (1, .4);
  \node[draw=black,rectangle] (z) at (2.5, .5) {$z = w^Tx + b$};
  \node at (2.5,-0.25) {\color{orange}$\d z$};
  \draw[->] (z) to (4.25, .5);
  \node[draw=black,rectangle] (yhat) at (5.5, .5) {$\hat y = a = \sigma(z)$};
  \node at (5.5,-0.25) {\color{orange}$\d a$};
  \draw[->] (yhat) to (7.25, .5);
  \node[draw=black,rectangle] (loss) at (8, .5) {$\mathcal L (a,y)$};
\end{tikzpicture}   \end{figure}
\newline
Note that $\d z = \d a \cdot g'(z)$ where $g(z) = \sigma(z)$, i.e. we use the fact that $\frac{\d L(a,y)}{\d z} = \frac{\d L(a,y)}{\d a} \frac{\d a}{\d z}$.

\paragraph{Two Layer Neural Network Back Propagation}
Let us now reconsider our two layer neural network and 
its computation graph. We highlight terms we must compute in \color{orange}{orange}\color{black}.
\begin{figure}[h]
\hspace{-20pt}
\begin{tikzpicture}
  \node (x)  at (-3, 1)  {x};
  \node (w1) at (-3, .5) {$W^{[1]}$};
  \node (b1) at (-3,0)   {$b^{[1]}$};
  \node [left = .1cm of w1] {\color{orange}$\d W^{[1]}= \d z^{[1]} x^T$}; 
  \node [left = .25cm of b1] {\color{orange}$\d b^{[1]}= \d z^{[1]}$}; 
  \node[draw=black,rectangle, right = .5cm of w1] (z1)  {$z^{[1]} = W^{[1]}x + b^{[1]}$};
  \draw[->] (x) to (z1);
  \draw[->] (w1) to (z1);
  \draw[->] (b1) to (z1);
  \node [below = .1cm of z1] {\color{orange}$\d z^{[1]}=W^{[2]T} \d z^{[2]} * g^{[1]'}(z^{[1]})$};
  \node[draw=black,rectangle, right = .5cm of z1] (a1) {$a^{[1]} = \sigma(z^{[1]})$};
  \draw[->] (z1) to (a1);
  \node [below = .1cm of a1] {\color{orange}$\d a^{[1]}$};
  \node[draw=black,rectangle, right = .5cm of a1] (z2) {$z^{[2]} = W^{[2]} a^{[1]} + b^{[2]}$};
  \node[below=.1cm of z2] (dz2) {\color{orange} $\d z^{[2]}= a^{[2]} - y$};
  \draw[->] (a1) to (z2);
  \node[above left = 1cm of z2] (w2) {$W^{[2]}$};
  \node[left = .1cm of w2] (dw2) {\color{orange}$\d W^{[2]}= \d z^{[2]} a^{[1]T}$};
  \node[below = .00025cm of w2] (b2) {$b^{[2]}$};
  \node[left = .1cm of b2] (db2) {\color{orange}$\d b^{[2]} = \d z^{[2]}$};
  \draw[->] (w2) to (z2);
  \draw[->] (b2) to (z2);
  \node[draw=black,rectangle,right = .5cm of z2] (a2) {$a^{[2]} = \sigma(z^{[2]})$};
  \node[below=.1cm of a2] (da2) {\color{orange} $da^{[2]}$};
  \draw[->] (z2) to (a2);
  \node[draw=black,rectangle,right=.5cm of a2] (loss) {$\mathcal{L}(a^{[2]}, y)$};
  \draw[->] (a2) to (loss);
\end{tikzpicture}   
\end{figure}
\newline
Where in $\d z^{[1]}=W^{[2]T} \d z^{[2]} * g^{[1]'}(z^{[1]})$ the $*$ denotes an element-wise product.
Note that the computations for $\d a^{[\ell]}$ are typically rolled into the next computation for $\d z^{[\ell]}$.
Further realize that the above computations are performed for \emph{each} training example in our data set 
during gradient descent.
\paragraph{Vectorized Implementation}
We can vectorize our computations as follows.
\begin{align*}   \d Z^{[2]} &= A^{[2]} - Y \\
  \d W^{[2]} &= \frac{1}{m} \d Z^{[2]} {A^{[1]}}^T \\
  \d b^{[2]} &= \frac{1}{m} \texttt{np.sum}(\d Z^{[2]}\texttt{, axis = 1, keepdims = True}) \\
  \d Z^{[1]} &= {W^{[2]}}^T \d Z^{[2]} * g^{[1]'}(Z^{[1]}) \hspace{35pt} (* \textrm{ denotes an element-wise product})\\
  \d W^{[1]} &= \frac{1}{m} \d Z^{[1]} X^T \\ 
  \d b^{[1]} &= \frac{1}{m} \texttt{np.sum}(\d Z^{[1]}\texttt{, axis = 1, keepdims = True}) \end{align*}

\subsubsection{Random Initialization}
\paragraph{Why Initializing With Zeros Won't Work for $W$'s: Symmetric Neurons}
Note that initializing our weights matrix with zeros will prevent gradient descent from working. What happens? For any example, $a_1^{[1]} = a_2^{[2]}$, whence $\d z_1^{[1]} = \d z_2^{[2]}$. I.e. the hidden units are symmetrical, and $\d W = \begin{bmatrix}   u & v \\ u & v \end{bmatrix}$ and our weights matrix will have first row equal to the second row. By induction, no matter how many times we train our network, the neurons in the hidden layer will remain symmetric and no learning will take place. 
It turns out $b$ does not have this symmetry-breaking problem, and so we can initialize $b$'s to zeros.

\paragraph{Random Initialization}
Instead, if we initialize weights randomly, each neuron in each hidden layer will compute different functions. I.e. $W^{[1]} = $\texttt{np.random.randn((.,.)) * 0.01} where we apply an ad-hoc scaling factor. We prefer initializing weights to small values. Why? Suppose we are using either sigmoid or $\tanh$ activation functions then if $W$ initialized large, $Wx + b$ also large, whence $g(z)$ large and we yield saturated (near-zero) gradients; this means gradient descent and learning will be very slow. If we don't have any sigmoid or $\tanh$ activation functions, this is less of an issue. 

\section{Deep Learning: $L$-layer Neural Networks}

Let's consider a four layer neural network, depicted below in figure \ref{fig: fourlayernet}.
Let us define some notation. Let $L=4$ denote the number of layers in the network. The notation $n^{[\ell]} = \# \textrm{units in layer } \ell$.

\begin{figure}[h]   \centering
  \label{fig: fourlayernet}
  \begin{tikzpicture}[scale=0.25]     \node (x1) at (-2, 1) {$x_1$};
    \node [below = .5 cm of x1] (x2) {$x_2$};
    \node [below = .5 cm of x2] (x3) {$x_3$};
    \node [draw,circle,above right = 1 cm of x1] (n11) {};
    \node [draw,circle,below = .5 cm of n11] (n12) {};
    \node [draw,circle,below = .5cm of n12] (n13) {};
    \node [draw,circle,below = .5cm of n13] (n14) {};
    \node [draw,circle,below = .5cm of n14] (n15) {};
    \node [draw,circle,right = 1 cm of n11] (n21) {};
    \node [draw,circle,below = .5 cm of n21] (n22) {};
    \node [draw,circle,below = .5cm of n22] (n23) {};
    \node [draw,circle,below = .5cm of n23] (n24) {};
    \node [draw,circle,below = .5cm of n24] (n25) {};
    \node [draw,circle,right = 1cm of n22] (n31) {};
    \node [draw,circle,below = .5cm of n31] (n32) {};
    \node [draw,circle,below = .5cm of n32] (n33) {};
    \node [draw,circle,right = 1cm of n32] (outputneuron) {};
    \node [right = 1cm of outputneuron] (yhat) {$\hat y$};
    \draw[->] (x1) to (n11);
    \draw[->] (x1) to (n12);
    \draw[->] (x1) to (n13);
    \draw[->] (x1) to (n14);
    \draw[->] (x1) to (n15);
    \draw[->] (x2) to (n11);
    \draw[->] (x2) to (n12);
    \draw[->] (x2) to (n13);
    \draw[->] (x2) to (n14);
    \draw[->] (x2) to (n15);
    \draw[->] (x3) to (n11);
    \draw[->] (x3) to (n12);
    \draw[->] (x3) to (n13);
    \draw[->] (x3) to (n14);
    \draw[->] (x3) to (n15);
    \draw[->] (n11) to (n21);
    \draw[->] (n11) to (n22);
    \draw[->] (n11) to (n23);
    \draw[->] (n11) to (n24);
    \draw[->] (n11) to (n25);
    \draw[->] (n12) to (n21);
    \draw[->] (n12) to (n22);
    \draw[->] (n12) to (n23);
    \draw[->] (n12) to (n24);
    \draw[->] (n12) to (n25);
    \draw[->] (n13) to (n21);
    \draw[->] (n13) to (n22);
    \draw[->] (n13) to (n23);
    \draw[->] (n13) to (n24);
    \draw[->] (n13) to (n25);
    \draw[->] (n14) to (n21);
    \draw[->] (n14) to (n22);
    \draw[->] (n14) to (n23);
    \draw[->] (n14) to (n24);
    \draw[->] (n14) to (n25);
    \draw[->] (n15) to (n21);
    \draw[->] (n15) to (n22);
    \draw[->] (n15) to (n23);
    \draw[->] (n15) to (n24);
    \draw[->] (n15) to (n25);
    \draw[->] (n21) to (n31);
    \draw[->] (n21) to (n32);
    \draw[->] (n21) to (n33);
    \draw[->] (n22) to (n31);
    \draw[->] (n22) to (n32);
    \draw[->] (n22) to (n33);
    \draw[->] (n23) to (n31);
    \draw[->] (n23) to (n32);
    \draw[->] (n23) to (n33);
    \draw[->] (n24) to (n31);
    \draw[->] (n24) to (n32);
    \draw[->] (n24) to (n33);
    \draw[->] (n25) to (n31);
    \draw[->] (n25) to (n32);
    \draw[->] (n25) to (n33);
    \draw[->] (n31) to (outputneuron);
    \draw[->] (n32) to (outputneuron);
    \draw[->] (n33) to (outputneuron);
    \draw[->] (outputneuron) to (yhat);
  \end{tikzpicture}
  \caption{Four-layer, fully connected neural network.} \end{figure}
In our example depicted, $n^{[0]} = n_x = 3$, $n^{[1]} = n^{[2]} = 5$ and $n^{[3]} = 3$ and $n^{L} = 1$. We'll also use $a^{[\ell]} = g^{[\ell]}(z^{[\ell]})$ to denote the activations in layer $\ell$, where to compute $z^{[\ell]}$ we use weights matrix $W^{[\ell]}$ and bias term $b^{[\ell]}$. Lastly, realize that with this notation our input matrix $X = a^{[0]}$ and our output $\hat y = a^{[L]}$.

\paragraph{Forward Propagation}
In general, we have that for a single training example:
\begin{align*}   z^{[\ell]} &= W^{[\ell]} a^{[\ell-1]} + b^{[\ell]}, \\ 
  a^{[\ell]} &= g^{[\ell]}(z^{[\ell]}). \end{align*}
And for our entire training set, where $X = A^{[0]}$ is our data arranged such that each column is an observation:
\begin{align*}   Z^{[\ell]} &= W^{[\ell]} A^{[\ell-1]} + b^{[\ell]}, \\ 
  A^{[\ell]} &= g^{[\ell]}(Z^{[\ell]}).    \end{align*}
Note that by rules of matrix multiplication, it's easy to see that $Z^{[\ell]}$ must have as many columns as there are training examples, and since $g^{[\ell]}$ is applied element-wise then $A^{[\ell]}$ also has as $m$ columns.
For our last layer, $\hat Y = g(Z^{[L]}) = A^{[L]}$. We remark that although we typically try to vectorize code and remove \texttt{for}-loops, it is perfectly reasonable to loop over the layers in the network during forward propagation. I.e. we expect to see \texttt{for} $\ell = 1, \ldots, L$.

\paragraph{Getting Your Dimensions Right} In general, the dimension for a weights matrix at layer $\ell$ is given b
\begin{equation}   W^{[\ell]} : \left( n^{[\ell]}, n^{[\ell - 1]} \right). \end{equation}
What about our bias term? In general, $b^{[\ell]} : \left(n^{[\ell]}, 1\right)$, Further, during backpropogation, the dimensions of the derivative matrices don't change. I.e. 
$\d   W^{[\ell]} : \left( n^{[\ell]}, n^{[\ell - 1]} \right)$, and $\d b^{[\ell]} : \left(n^{[\ell]}, 1\right)$.

\paragraph{Deep Representations:
\small What do our intermediary layers come to learn in different applications?}
\begin{itemize} \item Consider face detection. The first layer of the network may learn to detect edges, and the second layer perhaps may learn how to detect different pieces of a face by connecting edges together such as an eye or an ear. A later layer of a network may learn to distinguish between faces at an even less granular level.

\item As another example consider problems in the context of audio. The first layer of the network may lrn to distinguish between low and high frequency waveforms, whereas the second layer may learn to recognize phonemes (in the word cat, each letter, when pronounced, is a phoneme), and later layers of the network may learn to recognize words and finally phrases/sentences.
\end{itemize}
Earlier layers are in general based on more granular features, i.e. simple transformations of the input data. Later layers compose hierarchically and are eventually able to solve complex tasks.   
\paragraph{Circuit Theory and Deep Learning}
``There are functions you can compute with a `small' $L$-layer deep neural network that shallower networks require exponentially more hidden units to compute.''

As an example, consider using a neural network to approximate \texttt{xor} applied to $n$ input bits. I.e. we seek to calculate $x_1 \textrm{ xor } x_2 \textrm{ xor } \ldots \textrm{ xor } x_n$. If we are free to use as many layers in the network as we wish, then we could envision building a \texttt{xor} tree as follows: pair successive inputs together and apply the \texttt{xor} operation, and pipe the resulting output into the subsequent layer of the network. Realize that we require $\log n$ layers in our network in order to exactly compute the right answer. If, on the other hand, we weren't allowed to use an arbitrary number of layers but instead only a single hidden layer, we would require $O(2^n)$ neurons in order to achieve the same result; to see why realize that we effectively need to enumerate all possible $2^n$ possible input sequences in order to map them correctly back to the right answer in a subsequent step. Note that it is possible to get away with using only $2^{n-1}$ neurons to accomplish this task but this is still exponentially more neurons than the network with $O(\log n)$ layers.

\paragraph{Hyperparameters}
The \emph{parameters} of a neural network are the weights and biases:
\[
W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]}, \ldots, W^{[L]}, b^{[L]}. 
\]
But there are other factors which determine how these parameters are learned, and these in turn are called \emph{hyperparameters}. These include:
\begin{itemize} \item The learning rate $\alpha$.
\item The number of iterations of gradient descent that we run.
\item The number of hidden layers $L$.
\item The number of hidden units $n^{[1]}, n^{[2]}, \ldots, n^{[L]}$.
\item The choice of activation functions $g^{[1]}, g^{[2]}, \ldots, g^{[L]}$. \end{itemize}

In more advanced deep learning, additional hyperparameters include momentum, minibatch-size, and various forms of regularizations.
Applied deep learning is a very empirical process, which means that there is a loop: (i) idea, (ii) code, (iii) experiment; we iterate on this to find the best set of hyperparameters. It's nearly impossible to get these right on the first try, and so it's important that we set up a train, development, and test data set accordingly. The optimal configuration is often application/domain dependent.

\section{Setting up your Machine Learning Application}
\subsection{Splitting Data, and the (missing) Bias and Variance tradeoff}
\paragraph{Train, Development, and Test Data}
Historically, it was considered best practice to split data into training and test sets of size 70\% and 30\% respectively. This maybe makes sense when we have (up to) a million records. But with bigger data sets, we don't necessarily also need larger test sets in order to validate our algorithm; it may suffice to sample 10k records and use them as hold-out or test sets. If we believe in this example that 10k records is enough to deliver an unbiased estimate of which \emph{algorithm} is better (on the development/CV/hold-out set) and what the test error is (on the test set), then our split becomes more like 98\%, 1\%, 1\%. Lastly, we remark that it may be OK to not have a test set so long as we don't need an unbiased estimator of performance.
\vspace{-.5ex}
\paragraph{Bias - Variance} In Deep Learning, we still talk about bias and variance, but we don't emphasize as much that there is a tradeoff between these two quantities. Andrew considers \emph{bias} to be determined by the training set error relative to how well a human can perform the same task (i.e. Bayes error), and the \emph{variance} to be determined by the relation to the training set error. 
E.g. if \{\texttt{train, test}\} error is \{0.01, 0.11\} on a problem where humans can achieve perfect accuracy, we might say the algorithm has low bias but high variance. If we observe \{0.15, 0.16\}, we might say the algorithm has high bias. If we observe \{0.15, 0.30\}, we have a case of both high bias and high variance. Lastly, \{0.005, 0.01\} corresponds to both low bias and low variance.

\paragraph{Basic Recipe for Machine Learning}
Supposing that we have:
\begin{itemize}   \item \emph{High Bias}, as measured by the \emph{training} set. Possible remedies $\leadsto$ bigger network, more hidden units, train longer, and neural network architecture search.
  \item \emph{High Variance}, as measured by the \emph{development} set. Possible remedies $\leadsto$ more data, regularization, and neural network architecture search. \end{itemize}
Notice that depending on whether we have high bias or high variance, the set of things to try is very different. In modern deep learning, so long as we can train a bigger network on a bigger dataset this guarantees a model with lower bias \emph{and} lower variance, i.e. there is no longer a bias-variance tradeoff.

\subsection{Regularization} 
Suppose you have high variance. One way to address this is through regularization (another is through obtaining more training data, this may be harder but also more reliable). 
\paragraph{Logistic Loss with Regularization}
In this context, an $\ell_2$ regularization may look as follows:
\[
  J(w,b) = \frac{1}{m} \sum_{i=1}^m \mathcal L \left(\hat {y^{(i)}}, y^{(i)}\right) + \frac{\lambda}{2m} \|w\|_2^2
\]
where $\|w\|_2^2 = \sum_{j=1}^{n_x} w_j^2 = w^Tw$. Why not add regularization to $b$? Since $b \in \mathbb R$ just a single scalar, adding it to $w \in \mathbb R^{n_x}$ won't make much of a difference in practice ($w$ contains many real numbers whose magnitude overwhelm $b$). An $\ell_1$ regularization would look like $\frac{\lambda}{m} \sum_{i=1}^{n_x} |w_i|$. With $\ell_1$ regularization, we yield a \emph{sparse} model.
In all the above, $\lambda$ is the regularization hyper-parameter which is set using the cross-validation set.

\paragraph{Neural Network Regularization} Here,
\[
J\left(W^{[1]}, b^{[1]}, \ldots, W^{[L]}, b^{[L]}\right) = \frac{1}{m} \sum_{i=1}^{m} \mathcal L(\hat {y^{(i)}}, y^{(i)}) + \frac{\lambda}{2m} \sum_{\ell=1}^{L} \|W^{[\ell]}\|_F^2
\]
where we're using Frobenius norm $\|W^{[\ell]}\|_F^2 = \sum_{i=1}^{n^{[\ell-1]}} \sum_{j=1}^{n^{[\ell]}} (W_{ij}^{[\ell]})^2$, recalling that $W \in \mathbb R^{n^{[\ell]} \times n^{[\ell - 1]}}$. We need to make note that we have a new rule for back-propagation:
\[
\d W^{[\ell]} = \left(\texttt{from backprop}\right) + \frac{\lambda}{m} W^{[\ell]}.
\]
We sometimes refer to adding this regularization term as \emph{weight decay}, since the back propagation update rule becomes
\[
W^{[\ell]} := W^{[\ell]} - \alpha \d W^{[\ell]} = \underbrace{W^{[\ell]} - \frac{\alpha \lambda}{m} W^{[\ell]}}_{(1-\frac{\alpha \lambda}{m})W^{[\ell]}} - \alpha \left(\textrm{from backprop}\right)
\]
where $(1-\frac{\alpha \lambda}{m}) < 1$ is a shrinkage term.

\paragraph{Why does Regularization Mitigate Overfitting?} As we crank up $\lambda$, we push $W \leadsto 0$, whence the effects of hidden units are mitigated, and we realize a network that behaves closer to a stacked (multiple layers) logistic regression. 
Another way to gain intuition is as follows. Suppose we use a $\tanh$ activation function. If $\lambda \uparrow$, then $W^{[\ell]} \downarrow$, whence our input values to $\tanh$ are very small. Recall that $\tanh$ is approximately linear around the origin, and so for high levels of $\lambda$ we recover a neural network where every layer is linear; recall that no matter how deep the network is if it only consists of linear layers then the whole network yields a linear decision boundary.

\paragraph{(Inverted) Dropout Regularization} For \emph{each} training example, and for \emph{each} neuron in our network, we toss a (weighted) coin and remove the neuron from the network entirely according to the 
outcome. This means that for each training example we perform back propagation on a modified and (sometimes significantly) 
reduced network.
One way to implement dropout is known as ``inverted dropout''. We illustrate with layer $\ell = 3$, for some \texttt{keep\_prob} 
e.g. 0.8.

\begin{verbatim} 
d3 = np.random.rand(a3.shape[0], a3.shape[1]) < keep_prob  # rand, NOT randn.
a3 = np.multiply(a3, d3)
a3 /= keep_prob  # <-- Inverted dropout technique.
\end{verbatim}
The last step is what gives the technique its name, and is 
simply done in order to keep, in expectation, $z^{[4]} = W^{[4]} a^{[3]} + b^{[b]}$ the same. Note that this Boolean array \texttt{d3} is also used in back-propagation. 
Now, if we are to make predictions given a new $X = a^{[0]}$, \emph{we don't need to   apply drop-out at test time.}

\paragraph{Why does dropout work?} One intuition is that in applying dropout, we prevent our network from relying on any one particular feature too much and instead spread out the weights. We can think about this from the perspective of a single neuron in a network: since the neurons in the previous layer can be randomly eliminated, the neuron in the subsequent layer must learn not to place too much weight on any one feature and instead learn a small amount of information from each. This spread of weights has the effect of shrinking the squared norm of the weights matrix; in fact, dropout can be formally shown tobe an adaptive form of $\ell_2$ regularization.

\paragraph{Varying dropout by layer}
Note that it's also possible to vary the \texttt{keep\_prob} variable by layer. In fully connected layers with more neurons where we might worry about overfitting more, it's sensible to apply a more stringent (lower) value of \texttt{keep\_prob} when compared to perhaps a succession of layers with decreasingly fewer neurons where perhaps we are worried about overfitting less. The downside of this approach is that we have additional hyperparameters to search through when cross validating our approach; a workaround is to make a binary decision of whether to apply dropout at all to a given layer, and for all layers with dropout applied to use the same \texttt{keep\_prob}. We lastly remark that dropout \emph{can} be applied to the input features, but typically if any is applied we choose \texttt{keep\_prob} $\approx 1$.
One downside to dropout is that our cost function $J$ is no longer well defined, and we are not guaranteed that $J$ will be monotone decreasing as a function of the number of iterations of gradient descent.

\paragraph{Backpropagation with Dropout} There are two steps to carry out for each layer which utilizes dropout.
\begin{itemize} \item We previously shut down some neurons during forward propagation with mask $D^{[\ell]}$; in back propagation, we have to shut down the same neurons by applying the same mask $D^{[\ell]}$ to $\d A^{[\ell]}$.
\item During forward propagation we had previously divided $A^{[\ell]}$ by \texttt{keep\_prob}, whence in backward propagation we must divide $\d A^{[\ell]}$ by \texttt{keep\_prob} again (since $\frac{\d\,}{\d x} c f(x) = c \frac{\d\,}{\d x} f(x)$). \end{itemize}

\subsection{Other Regularization Methods} 
\paragraph{Data augmentation} E.g. in computer vision we can flip, rotate, and zoom into images to increase the size of our training set. By applying random distortions, we can fake additional training example. Of course, an independent training example provides more information than a synthesized example. In recognizing hand-written text, we could also apply small rotations and random perturbations to the ``thickness'' of the lines drawn. 

\paragraph{Early Stopping}
Another technique of regularization is \emph{early stopping}, whereby optimize for training error loss but also evaluate our cross-validation dataset error each iteration, and stop training when development set error starts increasing. Why does this method work? In practice, we initialize $W$ to small random numbers, and as we train more iterations of gradient descent we learn larger weights; by stopping early, we are in turn applying a shrinkage effect and encouraging a smaller Frobenius norm on $W$. Andrew likes to keep the task of Optimizing Cost Function $J(W,b)$ separate from the task of Not Overfitting, since each has its own set of workarounds; see orthogonalization.

\section{Setting Up Your Optimization Problem}
\subsection{Normalizing input data}
Suppose we have a two dimensional data-set as follows; we plot out $(x_1, x_2)$ pairs.

\begin{tikzpicture}[     declare function={a(\x)=0.75;},     declare function={b(\x)=0.25;} ] \begin{axis}[     domain=1:5,     axis lines=middle,     axis equal image,     xtick=\empty, ytick=\empty,     enlargelimits=true,     clip mode=individual, clip=false ] \addplot [red, only marks, mark=*, samples=100, mark size=0.75]     {0.5*(a(x)+b(x)) + 0.5*rand*(a(x)-b(x))}; \end{axis} \end{tikzpicture}

We standardize our data by first subtracting out the mean: $\mu = \frac{1}{m} \sum_{i=1}^{m} x^{(i)}$, $x -= \mu$.

\begin{tikzpicture}[     declare function={a(\x)=0.25;},     declare function={b(\x)=-0.25;} ] \begin{axis}[     domain=-2:2,     axis lines=middle,     axis equal image,     xtick=\empty, ytick=\empty,     enlargelimits=true,     clip mode=individual, clip=false ] \addplot [red, only marks, mark=*, samples=100, mark size=0.75]     {0.5*(a(x)+b(x)) + 0.5*rand*(a(x)-b(x))}; \end{axis} \end{tikzpicture}

We then divide through by the variance, $\sigma^2 = \frac{1}{m} \sum_{i=1}^{m} x^{(i)}**2$ where we remark that the $**$ denotes element-wise multiplication and further that typically $\textrm{Var}(Z) = \mathbb E [Z^2] - \mathbb E [Z]^2$, but we've already de-meaned our random vector.
$x /= \sigma^2$.
\begin{tikzpicture}[scale=0.5,     declare function={a(\x)=0.25;},     declare function={b(\x)=-0.25;} ] \begin{axis}[     domain=-.5:.5,     axis lines=middle,     axis equal image,     xtick=\empty, ytick=\empty,     enlargelimits=true,     clip mode=individual, clip=false ] \addplot [red, only marks, mark=*, samples=100, mark size=0.75]     {0.5*(a(x)+b(x)) + 0.5*rand*(a(x)-b(x))}; \end{axis} \end{tikzpicture}

We remark that the same $\mu$ and $\sigma^2$ should be used to normalize the development and test data-sets.

\paragraph{Normalization and Gradient Descent} Why should we normalize our data? It comes down to being able to apply a higher learning rate when training our algorithm. When our input data $x_1, x_2$ have very different scales (e.g. $x_1 \in [0, 1e3]$ and $x_2 \in [0,1]$), the weights learned for $w_1$ and $w_2$ will be on very different scales as well. If we plot the loss function $\mathcal L(\hat y, y)$ over $w_1, w_2$ it turns out that we end up with very elongated contours. We therefore must apply a small learning rate, leading to many small steps before convergence.

\begin{minipage}{1.0\textwidth} \begin{multicols}{2} \begin{tikzpicture}
  \draw[->] (-3.25,-2.25) to (-3.25, 1);
  \node at (-3.25, 1.2) {$w_2$};
  \draw[->] (-3.25,-2.25) to (3.25, -2.25);
  \node at (3.5, -2.25) {$w_1$};
    \draw[rotate=30] (0,0) ellipse (3cm and 1.5cm); 
    \draw[rotate=30] (0,0) ellipse (2cm and 1cm);     \draw[rotate=30] (0,0) ellipse (1cm and .5cm); 
    \draw[rotate=30] (0,0) ellipse (.5cm and .25cm);
  \draw[->] (-2.5, -1) to (-1.8, -1.25);
  \draw[->] (-1.8, -1.25) to (-1.1, -.5);
  \draw[->] (-1.1, -.5) to (-.4, -.25);
  \draw[->] (-.4, -.25) to (0, 0);
\end{tikzpicture} 
\vfill\null \columnbreak
\begin{tikzpicture}[scale=2]
  \draw[->] (-1, -1) to (-1, 1);
  \node at (-1, 1.1) {$w_2$};
  \draw[->] (-1, -1) to (1, -1);
  \node at (1.25, -1) {$w_1$};
  \draw (0, 0) circle (1cm);   
  \draw (0, 0) circle (.5cm); 
  \draw (0, 0) circle (.25cm); 
  \draw[->] (-1, 0) to (-.5, 0);
  \draw[->] (-.5, 0) to (0, 0);
\end{tikzpicture}
\end{multicols} \end{minipage}

Whereas if we first normalize our inputs, the loss function when plotted over the weights appears more like a uniform bowl, and our contour plots are spherical. Our learning rate can be set higher since the step size is no longer limited by the major axis. Performing normalization never hurts our learning algorithm.

\subsection{Vanishing/Exploding gradients}
Suppose for the sake of argument that we have a very deep neural network; we've drawn only two hidden neurons per layer but this is only for illustration sake.

\begin{figure}[h]
\centering
\begin{tikzpicture}[darkstyle/.style={circle,draw,fill=gray!40,minimum size=20}]
  \node (x1) at (-1.3, 1.5) {$x_1$};   \node (x2) at (-1.3, 0) {$x_2$};   
  \foreach \x in {0,...,4}     \foreach \y in {0,...,1}
      \node [darkstyle] (\x\y) at (1.5*\x,1.5*\y) {};
  \draw[->] (x1) to (00);
  \draw[->] (x1) to (01);
  \draw[->] (x2) to (00);
  \draw[->] (x2) to (01);
  \draw[->] (00) to (10);
  \draw[->] (00) to (11);
  \draw[->] (01) to (10);
  \draw[->] (01) to (11);
  \draw[->] (10) to (21);
  \draw[->] (10) to (20);
  \draw[->] (10) to (21);
  \draw[->] (11) to (20);
  \draw[->] (11) to (21);
  \draw[->] (20) to (30);
  \draw[->] (20) to (31);
  \draw[->] (21) to (30);
  \draw[->] (21) to (31);
  \draw[->] (30) to (40);
  \draw[->] (30) to (41);
  \draw[->] (31) to (40);
  \draw[->] (31) to (41);
  \node[darkstyle] (outputneuron) at (7, .75) {}; 
  \draw[->] (40) to (outputneuron);
  \draw[->] (41) to (outputneuron);
  \node (yhat) at (8, .75) {$\hat y$};
  \draw[->] (outputneuron) to (yhat);
\end{tikzpicture}
\end{figure}

Suppose we choose $g(z) = z$, i.e. a linear activation function, and suppose for sake of argument that we ignore our bias term, i.e. $b^{[\ell]} = 0$ for each layer. It's easy to see that $\hat y = W^{[L]} W^{[L-1]} \ldots W^{[2]} W^{[1]} W^{[0]} X$. Ignoring the last $W^{[L]}$ which is of different dimension, suppose each intermediary weight matrix is slightly larger than an identity matrix i.e. $\begin{bmatrix}   1 + \epsilon & 0 \\ 0 & 1 + \epsilon \end{bmatrix}$ then the value of $\hat y$ explodes with the number of layers,
$\hat y = W^{[L]} \begin{bmatrix}   (1 + \epsilon)^{L-1} & 0 \\ 0 & (1 + \epsilon)^{L-1} \end{bmatrix}X$. Similarly, if the intermediary matrices are slightly smaller than an identity matrix, then the values of subsequent activation functions get exponentially smaller. This can start to become problematic if we have $L$ quite large; in 2018 Microsoft published state of the art results using $L \approx 150$, which is large enough for these problems to manifest by slowing down gradient descent.

\subsection{Weight Initialization for Deep Networks} Suppose for the sake of illustration we have a single activation neuron.
\begin{figure}[h]
  \centering   
  \begin{tikzpicture}[scale=0.6]
    \node (x1) at (-1, 1) {$x_1$};
    \node (x2) at (-1, 0) {$x_2$};
    \node (x3) at (-1,-1) {$x_3$};
    \node (x4) at (-1,-2) {$x_4$};
    \node[draw,circle] (neuron) at (1.5,0) {};
    \draw[->] (x1) to (neuron);
    \draw[->] (x2) to (neuron);
    \draw[->] (x3) to (neuron);
    \draw[->] (x4) to (neuron);
    \node (output) at (5,0) {$\hat y$};
    \draw[->] (neuron) to (output);
  \end{tikzpicture} 
\end{figure}
For simplicity sake, ignore $b$ for now. The input to our activation function
looks like $z = w_1 x_1 + w_2 x_2 + \ldots + w_n x_n$. So, if the number of input features (or hidden neurons in an intermediary layer) is large, we want smaller $w_i$ to compensate such that $z$ doesn't explode. One idea might be to set $\textrm{Var}(w_i) = \frac{1}{n}$ such that $\textrm{Var}(z) \approx \sum_i \textrm{Var}(x_i)$ and we preserve the variance of our input to our activation function. To accomplish this, we set
\begin{align*}   W^{[\ell]} = \texttt{np.random.randn(shape) * np.sqrt} \left(\frac{1}{n^{[\ell-1]}}\right) \end{align*}
If we're using ReLU activation functions, it's common to replace the numerator in the scaling constant above with a 2 instead of a 1 (i.e. to set $\textrm{Var}(w_i) = \frac{2}{n}$). If you want, you can consider this scaling factor to be another hyperparameter, and we can search across a sequence of values and choose the best according to our development data-set.

\subsection{Numerical Approximation of Gradients}
When performing back propagation, there is a technique known as Gradient-Checking
that will help to ensure your implementation of back propagation is correct. To 
perform gradient checking, we first need to understand how to numerically approximate
a gradient.

Suppose we have some function $f(\theta) = \theta^3$, and we choose $\epsilon = 0.01$. To compute a numerical gradient around a point $\theta$, we will use a central difference method:
\[
f'(\theta) \approx \frac{f(\theta + \epsilon) - f(\theta - \epsilon)}{2 \epsilon}.
\]
This approximation has an error that is upper-bounded by $O(\epsilon^2)$, which can be shown through a Taylor Series approximation (see \href{https://en.wikipedia.org/wiki/Finite_difference_method#Accuracy_and_order}{finite difference method}), recalling that $f'(\theta) = \lim_{\epsilon \to 0} \frac{f(\theta + \epsilon) - f(\theta - \epsilon)}{2 \epsilon}$. This is in stark contrast to taking a one-sided difference to approximate a gradient, which has error bounded by $O(\epsilon)$.

\paragraph{Gradient Checking} A technique for catching errors in a back-propagation implementation. The first step is to take
$W^{[1]}, b^{[1]}, \ldots, W^{[L]}, b^{[L]}$ and \emph{reshape} 
them into one giant vector, call it $\theta$. We may now 
equivalently define our cost function as 
 $J(W^{[1]}, b^{[1]}, \ldots, W^{[L]}, b^{[L]}) = J(\theta)$.
The next step is to take $\d W^{[1]}, \d b^{[1]}, \ldots, \d W^{[L]}, \d b^{[L]}$ (ordered the same way) and reshape them into another giant vector $\d \theta$.

\begin{algorithm}
  \caption{\texttt{Grad-check}}   \For{each $i$} {
    $\d \theta_{\texttt{approx}}^{[i]} = \frac{J(\theta_1, \theta_2, \ldots, \theta_i + \epsilon, \ldots) - J(\theta_1, \theta_2, \ldots, \theta_i - \epsilon, \ldots)}{2\epsilon}$
  } \end{algorithm}
We now end up with two vectors, each the same length, $\d \theta_{\texttt{approx}}^{[i]}$ and $\d \theta$. We can now compare them using Euclidean norms,
\begin{equation}
\label{eq: normed-diff}
\frac{\| \d \theta_{\texttt{approx}} - \d \theta\|_2}{\|\d \theta_{\texttt{approx}}\|_2 + \| \d \theta \|_2}
\end{equation}

Now, suppose we implement gradient checking with $\epsilon = 10^{-7}$. If \ref{eq: normed-diff} is $O(10^{-7})$ or smaller, then great! We've implemented back propagation correctly. If \ref{eq: normed-diff} outputs something $O(10^{-5})$, it would be prudent to check the components (elements) of $\d \theta_{\texttt{approx}} - \d \theta\|_2$ and see if any one of them is particularly large, if so we might want to double check our implementation for that layer. If, however, \ref{eq: normed-diff} outputs something $O(10^{-3})$, it's indicative of a bug in our implementation. Again, the method is to inspect the components of \ref{eq: normed-diff} to locate the source of our bug.

\paragraph{Implementation Notes for Gradient Checking}
\begin{itemize}   \item Don't use in training -- only to debug. I.e. computing $\d \theta_{\texttt{approx}}^{[i]}$ for each $i$ is expensive. Once we've verified that \ref{eq: normed-diff} outputs something sufficiently small, we should turn \emph{off} \texttt{grad-check}.
  \item If algorithm fails \texttt{grad-check}, examine individual components in order to debug. We might find that the bug is isolated to a particular layer. 
  \item Remember regularization -- if $J(\theta) = \frac{1}{m} \sum_{i=1}^m \mathcal L(\hat {y^{(i)}}, y^{(i)}) + \frac{\lambda}{2m} \sum_\ell \|W^{[\ell]}\|_F^2$ includes regularization, then we need to be sure to include this penalty term when we compute $\d \theta_{\texttt{approx}}$, since $\d \theta$ is defined as the derivative with respect to our cost function.
  \item \texttt{Grad-check} does \emph{not} work with dropout -- since dropout randomly eliminates (hidden) nodes from the network, there is no easy to compute $J(\theta)$ that dropout is doing gradient descent on.\footnote{It turns out that when using dropout, we can actually define a cost function that is defined over the exponentially large space of all possible subsets of hidden neurons, however this is computationally intractable to compute and compare with.} To get around this, we might turn \texttt{keep\_prop = 1.0} before running \texttt{grad-check}.
  \item Run at a random initialization of $W$'s and $b$'s, and perhaps again after some training -- it's possible that our (buggy) back propagation algorithm may not incur \emph{too} much error when $W, b \approx 0$, but that as our weights increase the errors do to. So, we might try running \texttt{grad-check} when we first randomly initialize $W$ and $b$, and then also again after some training has allowed our weights to drift sufficiently away from zero.  \end{itemize}

\section{Optimization Algorithms}
\subsection{Mini-batch gradient descent}
We've previously seen that matrix-multiplies allow us to efficiently compute a gradient on all $m$ training examples simultaneously. I.e., we had
\begin{equation}
  X = \begin{bmatrix}     x^{(1)} & x^{(2)} & \ldots & x^{(m)}   \end{bmatrix} \in \mathbb R^{n_x, m} \\ \hspace{15pt}
  Y = \begin{bmatrix}     y^{(1)} & y^{(2)} & \ldots & y^{(m)}   \end{bmatrix} \in \mathbb R^{1, m}.
\end{equation}

But in deep learning we often work with large data; 
what if $m$ really large, e.g. 50M? This means that we have to churn through a lot of compute before taking a very small step in gradient descent. It turns out that we can often allow our algorithm to learn faster if we allow for gradient descent to start taking palce before we're done processing all $m$ training examples.

We partition our data into mini-batches, 
each of approximately equal size. Let us use a superscript curly-brace to denote the mini-batch number. E.g. if we choose a mini-batch size of 1e3 training examples, 
our training data now looks like
\[
  X = \begin{bmatrix}       \color{purple} x^{(1)} & \color{purple} x^{(2)} & \color{purple} \ldots & \color{purple} x^{(1000)}      & \color{orange} x^{(1001)} & \color{orange} \ldots & \color{orange} x^{(2000)} & \color{black} \ldots & x^{(m)}   \end{bmatrix} \in \mathbb R^{n_x, m}
\]
and correspondingly for $Y$.
The number of mini-batches is of course given by $m/\texttt{mini-batch size}$. Mini-batch $t$ then consists of $\{X^{\{t\}}, Y^{\{t\}}\}$. The dimension of $X^{\{t\}}$ is $(n_x, \texttt{mini-batch size})$.

\begin{algorithm}[h]   \caption{Mini-batch gradient descent}
  \For{$t = 1, \ldots, m/\texttt{mini-batch size}$} {
    \tcp{One step (epoch) of gradient descent using $X^{\{t\}}, Y^{\{t\}}$.}
    $Z^{[1]} = W^{[1]} X^{\{t\}} + b^{[1]}$ \\
    $A^{[1]} = g^{[1]}(Z^{[1]})$ \\
    $\vdots$ \\
    $A^{[L]} = g^{[L]}(Z^{[L]})$
    $J^{\{t\}} \gets \frac{1}{\texttt{mini-batch size}} \sum_{i=1}^{\texttt{mini-batch size}} \mathcal L(\hat y^{(i)}, y^{(i)}) + \frac{\lambda}{2 \cdot \texttt{mini-batch size}} \sum_{\ell = 1}^{L} \|W^{[\ell]}\|_F^2$ \\
    \tcp{Backpropagation to compute gradients w.r.t. $J^{\{t\}}$ (using $X^{\{t\}}, Y^{\{t\}}$).}
    $W^{[\ell]} := W^{[\ell]} - \alpha \d W^{[\ell]}$
    $b^{[\ell]} := b^{[\ell]} - \alpha \d b^{[\ell]}$
  } \end{algorithm}

With larger data sets, mini-batch gradient descent is likely to converge much faster.

\subsubsection{Understanding mini-batch gradient descent}
With (vanilla) batch gradient descent, we expect that the loss
function should be monotone decreasing with respect to the 
number of iterations of gradient descent. E.g.
\begin{tikzpicture}
\draw[->] (0, 0) to (0, 1.1) node [left] {cost}; \draw[->] (0, 0) to (2.1, 0) node [right]{\# iterations}; 
\draw[smooth, domain=0.2:2, color=orange, thick]      plot (\x,{exp(-\x)}) {};
\end{tikzpicture}
When we train on a cost function $J^{\{t\}}$, it depends on
$X^{\{t\}}, Y^{\{t\}}$ which vary with each iteration. And so
it's not the case that we expect our cost function to 
monotonically decrease with each iteration of mini-batch gradient descent. We should see something that is (hopefully) sinusoidal and downward trending, e.g.
\begin{tikzpicture}
\draw[->] (0, 0) to (0, 1.1) node [left] {cost}; \draw[->] (0, 0) to (2.1, 0) node [right]{\# iterations};
\draw[smooth, domain=0.2:2, color=orange, thick]
plot (\x,{exp(-\x)*abs(cos(\x*1000))}) {};
\end{tikzpicture}. Why does our learning plot have this shape? It could be the case that the first epoch of data $X^{\{1\}}, Y^{\{1\}}$ was ``easier'' to predict on relative to a latter epoch.


\subsubsection{Choosing mini-batch size}
One point worth discussing is how to choose your mini-batch size. If we select $\texttt{mini-batch size} = m$, then we recover gradient descent. On the other extreme, we can set the \texttt{mini-batch size} to $1$, in which case we get 
\href{https://en.wikipedia.org/wiki/Stochastic_gradient_descent}{Stochastic Gradient Descent}, in which case each 
training example is its own mini-batch. Let's consider these two extremes as applied to the following hypothetical loss function (and corresponding contours).
{
\begin{figure}[h]
\centering
\begin{tikzpicture}
      \draw[->] (-3.25,-2.25) to (-3.25, 1);
      \node at (-3.25, 1.2) {$w_2$};
      \draw[->] (-3.25,-2.25) to (3.25, -2.25);
      \node at (3.5, -2.25) {$w_1$};
      \draw[rotate=30] (0,0) ellipse (3cm and 1.5cm); 
      \draw[rotate=30] (0,0) ellipse (2cm and 1cm);     \draw[rotate=30] (0,0) ellipse (1cm and .5cm); 
      \draw[rotate=30] (0,0) ellipse (.5cm and .25cm);
  \draw[->,orange] (-1.8, -2) to (-1.7, -1.05);
  \draw[->,orange] (-1.7, -1.05) to (-.9, -.5);
  \draw[->,orange] (-.9, -.5) to (-.4, -.25);
  \draw[->,orange] (-.4, -.25) to (0, 0);
  \NewDocumentCommand{\irregularline}{O {2mm} m m D <> {20}}{{       \coordinate (old) at #2;       \foreach \i in {1,2,...,#4}{         \draw (old) -- ($ ($#2!\i/(#4+1)!#3$) + (0,#1*rand) $) coordinate (old);       }       \draw (old) -- #3;     }}
  \irregularline{(0,0)}{(1.8,1.8)}<100>
\end{tikzpicture} 
\end{figure}
}

We've drawn vanilla gradient descent (in \color{orange}{orange}\color{black}) taking a few easy steps through the training data, each time moving toward the 
(global) minimum. On the other hand stochastic gradient descent (which we've drawn starting from a different initialization point for the sake of illustration) 
doesn't always get the direction right and in fact the path may be
far more circuitous than what we've shown. In fact, SGD won't even ever hit the global minimum (or ``converge''), it is far more likely to simply ``dance around'' the global minimum within some reasonably tight radius.

\paragraph{Advantages of mini-batch gradient descent}
In reality, we choose a mini-batch size in between 1 and $m$. If we were to choose $m$, the time required for each step of 
training would be too large (assuming large data). If we were to choose SGD, the downside is that we lose almost all speed-up from 
vectorization, since we process only one example at a time. With mini-batch gradient descent, we do get some 
vectorization (speed-up relative to processing one example at a time), and secondly we can make progress without 
waiting to process the entire training set. Further, with mini-batch gradient descent we won't take as circuitous a 
route before approaching the global optimum (which again we will not hit exactly).

\begin{itemize}   
  \item If training set is small (e.g. less 2,000 observations), just use batch gradient descent since the whole training set can be processed quite quickly.
  \item Otherwise, consider picking a mini-batch size equal to a power of two, such that hopefully data can 
    be loaded and stored in cache (or GPU memory), perhaps in the set $\{2^6, \ldots, 2^9\}$. 
\end{itemize}

\subsection{Momentum and Exponentially Weighted (moving) Averages} There are faster algorithms relative to mini-batch gradient descent.
To understand them, we need to understand \href{https://en.wikipedia.org/wiki/Moving_average#Exponential_moving_average}{exponentially weighted averages}. Suppose we have a time-series dataset  $x_1, \ldots, x_T$; we might estimate a moving average as follows:
\begin{align}
  \label{eq: expmovavg}   v_0 &= 0 \nonumber \\
  v_i &= 0.9 * v_{i-1} + 0.1 * x_i \hspace{35pt} \texttt{for } i > 0. \end{align}

We can make this a little bit more interesting by choosing some $\beta$ and then setting our update rule to become $v_i = \beta * v_{i-1} + (1-\beta) * x_i$. It can be shown that $v_t$ is an average over approximately $\frac{1}{1-\beta}$ points of data. As we set $\beta \approx 1$, we average over more points of data and get a \emph{smoother} time-series. The downside of averaging over more points is that we learn the trend of our data \emph{slower} (and therein incur a higher latency bias).

\paragraph{Math behind exponentially weighted averages} Let's unroll our formula to 
gain an intuition for what's happening. Suppose we have 100 data points.
\begin{align*}   v_{100} &= \beta v_{99} + (1-\beta) x_{100} \\
  v_{99}  &= \color{purple} \beta v_{98} + (1-\beta) x_{99} \\ 
  &\ldots \end{align*}
Then we can rewrite our last value as 
\[
  v_{100} = (1-\beta) x_{100} + \beta \left( \color{purple} (1-\beta) x_{99} + \beta v_{98} \color{black} \right) 
\]
and continuing to unroll term-by-term we see that
\[
  v_{100} = (1-\beta) x_{100} + (1-\beta) \beta x_{99} + (1-\beta) \beta^2 x_{98} + (1-\beta) \beta^3 x_{97} + \ldots.
\]

Put differently, the time series of data-points $v_i$ are generated by taking the element-wise product of our original time series with an exponentially decaying function which places the most weight on the most recent data point (and decaying weight to older data points). The coefficients add up to one, less a bias term that will be discussed later. In practice, to compute \ref{eq: expmovavg}- we only need to store a single real number in memory which we continuously update to take on the latest value of 
the moving average (based on its last value and the most recent data).

\paragraph{Bias correction in exponentially weighted (moving) averages} Let's consider our recurrence \ref{eq: expmovavg}. Notice that since we initialize $v_0 = 0$, and since $\beta \in (0,1)$, then
\begin{align*}   v_1 = \underbrace{\beta v_0}_{=0} + (1-\beta) x_1 \implies v_1 < x_1
%  v_2 = \beta v_1 + (1-\beta) x_2 = \beta (1-\beta) x_1 + (1-\beta) x_2 = (1-\beta) \left(\beta x_1 + x_2\right)
\end{align*}
we see that we incur a bias in our first point estimate. This persists until our moving average ``learns'' enough from the past data. To make our estimate more accurate during the initial phase, we take $v_t$ and scale it as follows:
\[
  \frac{v_t}{1 - \beta^t}.
\]
When $t$ is large, this bias correction fades away, but when $t$ is small it can really help.

\paragraph{Gradient descent with momentum} The idea is to compute an exponentially weighted average of gradients, and to use this to update our weights; it's almost always faster when compared with vanilla gradient descent. As an example, let's consider the following cost function with corresponding contours; starting from an arbitrary point, we might expect (mini) batch gradient descent to behave in the following way.

\begin{figure}[h]
\centering
\begin{tikzpicture}
  \draw[->] (-3.25,-2.25) to (-3.25, 1);
  \node at (-3.25, 1.2) {$w_2$};
  \draw[->] (-3.25,-2.25) to (3.25, -2.25);
  \node at (3.5, -2.25) {$w_1$};
  \draw (0,-1) ellipse (3cm and .75cm); 
  \draw (0,-1) ellipse (2cm and .625cm);     
  \draw (0,-1) ellipse (1.25cm and .5cm); 
  \draw (0,-1) ellipse (.75cm and .375cm);
  \node[fill,red,circle,scale=0.5] at (0,-1) {};
  \draw[smooth, domain=-2.75:-.00125, color=blue, thick]
    plot (\x,{1/7*abs(\x)*sin((\x-3)*1250)-1}) {};
  \draw[->,purple,dashed] (2.5,-1) to (2.25, 0);
  \draw[->,purple,dashed] (2.25,0) to (2, -1.75);
  \draw[<->] (4, .25) to (4, -.25);
  \node at (6.75, 0) {\texttt{slower learning rate}};
  \draw[<->] (3.75, -.5) to (4.25, -.5);
  \node at (6.75, -.5) {\texttt{faster learning rate}};
  \draw[->,orange,dashed] (-2.75,-1) to (-2, -.75);
  \draw[->,orange,dashed] (-2, -.75) to (-1, -1.1);
  \draw[->,orange,dashed] (-1, -1.1) to (0, -1);
\end{tikzpicture}
\end{figure}

Notice that initially in this gradient descent trajectory, 
we tend to end up ``on the other side of the 
ellipse'' than intended, this process repeats and 
we end up \emph{oscillating} toward the minimum.
If we try to use a \emph{larger} learning rate, we are likely
to \emph{diverge} (pictured in \color{purple}{purple} 
above\color{black}).
This forces us to use a learning rate that is not too large.
Another way to view this problem, is that on the vertical 
axis, we'd like slower learning, but on the horizontal axis 
we'd like faster learning. This is because the shape of the contours make it easier to ``overshoot'' and end up with a larger cost value when moving up and down relative to left and right.

\paragraph{Momentum} On iteration $t$, we compute $\d W$, $\d b$ on current (mini)-batch. Then, compute
\begin{align*}
V_{\d W} &= \beta V_{\d W} + (1-\beta) \d W \\
V_{\d b} &= \beta V_{\d b} + (1-\beta) \d b
\end{align*}
Our update rule then becomes
\begin{align*}
W &:= W - \alpha V_{\d W} \\
b &:= b - \alpha V_{\d b}
\end{align*}
This has the effect of ``smoothing out'' our steps of gradient descent and in particular it dampens the oscillations that we previously saw. Why is this? Well, if we previously had many up and down oscillations, the average gradient is near zero, whereas on the other hand we are always moving consistently toward the global minimum from left to right and so the weighted average still pulls strongly in this direction; we've plotted this in \color{orange}{orange} \color{black} above.

\paragraph{Intuition} Suppose we have a bowl shaped loss function, and that we plan to roll a ball ``downhill'' toward the global minimum. The terms $\d W$ and $\d b$ describe \emph{acceleration} that we place on the ball, whereas the terms $\beta V_{\d W}$ and $\beta V_{\d b}$ describe \emph{velocity} and in particular wince 
$\beta < 1$ we have a notion of \emph{friction} which prevents
our ball from rolling too fast or out of control.

\paragraph{Hyperparameters} Notice that we've added an additional hyperparameter $\beta$ which controls the exponentially weighted average. The most common value Andrew Ng reports for $\beta$ is 
0.9, which corresponds to averaging over the last $1/(1-.9) = 10$ 
gradients.

\subsection{RMSProp} Momentum was one method of speeding up gradient descent. There's another known as Root Mean Square Prop. On iteration $t$ of (mini) batch gradient descent, we compute the following (where here $\cdot^2$ denotes an element-wise squaring operation.
\begin{align*}   S_{\d W} &= \beta S_{\d W} + (1-\beta) \d W^2 \\ %\hspace{15pt} \tcp{Element-wise square}
  S_{\d b} &= \beta S_{\d b} + (1-\beta) \d b^2 \\ %\hspace{15pt} \tcp{Again, element-wise square}
  W &:= W - \alpha \frac{\d W}{\sqrt{S_{\d W}} + \epsilon} \\
  b &:= b - \alpha \frac{\d b}{\sqrt{S_{\d b}} + \epsilon} \end{align*}

Adam stands for Adaptive Moment Estimation. $\beta_1$ is used to compute the mean of the gradients $\d W$ (first moment) and $\beta_2$ is used to compute the mean of the second moments $\d W^2$.

\paragraph{Intuition}
Notice that it's in the update step where a RMS term appears.
Why does this work? Recall our previous example where in the vertical direction we wanted a smaller learning rate, but for the horizontal direction we wanted a larger learning rate. By incorporating the square of the gradient, we can achieve similar outcomes as with momentum (\emph{dampening gradients that oscillate wildly}).

\paragraph{Implementation Details} In practice, we add some $\epsilon \approx 1e-8$ to the denominator such that we don't divide by something too close to zero (causing our update to blow up).

\subsection{Adam} This is a rare instance of an algorithm that actually works; it works by combining momentum with RMSProp. We first initialize parameters:
\begin{align*}   V_{\d W} = 0, S_{\d W} = 0, V_{\d b} = 0, S_{\d b} = 0 \end{align*}
Then, on iteration $t$, we compute 
$\d W$, $\d b$ using current mini-batch, and then
\begin{align*}   V_{\d W} &= \beta_1 V_{\d W} + (1-\beta_1) \d W \\ 
  V_{\d b} &= \beta_1 V_{\d b} + (1-\beta_1) \d b \\ 
  S_{\d W} &= \beta_2 S_{\d W} + (1-\beta_2) \d W^2 \\
  S_{\d b} &= \beta_2 S_{\d b} + (1-\beta_2) \d b^2 \end{align*}
Typically, we do implement Adam by using $V^{\texttt{corrected}}_{\d W} = \frac{V_{\d W}}{1-\beta_1^t}$ and $V^{\texttt{corrected}}_{\d b} = \frac{V_{\d b}}{1 - \beta_1^t}$ and similarly we have
$S^{\texttt{corrected}}_{\d W} = \frac{S_{\d W}}{1-\beta_2^t}$ and $S^{\texttt{corrected}}_{\d b} = \frac{S_{\d b}}{1-\beta_2^t}$. Then, we perform the updates
\begin{align*}   W &:= W - \alpha \frac{V^{\texttt{corrected}}_{\d W}}{\sqrt{S^{\texttt{corrected}}_{\d W}} + \epsilon} \\
  b &:= b - \alpha \frac{V^{\texttt{corrected}}_{\d b}}{\sqrt{S^{\texttt{corrected}}_{\d b}} + \epsilon} \end{align*}
\paragraph{Hyperparameters}
There are now multiple hyperparameters.
\begin{itemize}   \item $\alpha$ is a hyperparameter that typically needs to be tuned.
\item $\beta_1 \gets 0.9$, i.e. for $\d W$ we average over last ten gradients.
\item $\beta_2 \gets 0.999$, i.e. for $\d W^2$ we average over the last $\frac{1}{1 - 0.999} = 1,000$ gradients.
\item $\epsilon \gets 1e-8$ is typically fixed. \end{itemize}

\subsection{Learning Rate Decay} The idea here is that we take larger steps at first (making steady progress toward the global minimum), but as we approach we start taking smaller steps so as to avoid the problem of dancing around in circles a far region away from the global minimum. To implement learning rate decay, we may choose a heuristic where $\alpha = \frac{1}{1 + \texttt{decay\_rate}*\texttt{epoch\_\#}} \alpha_0$. As a function of the epoch number, this formula (dependent on hyperparameter $\alpha_0$) decays steadily. Another few formulae people try are
\begin{align*}   \alpha &= 0.95^{\texttt{epoch\#}} \cdot \alpha_0 \\
  \alpha &= \frac{k}{\sqrt{\texttt{epoch\#}}} \cdot \alpha_0 \\
  \alpha &= \frac{k}{\sqrt{t}} \cdot \alpha_0 \end{align*}

\subsection{Local Optima} As the field of deep learning has evolved, so has the way we think about local optima. People used to examine low (three) dimensional plots in which it's easy to create many local optima that are significantly worse than the global optimum. In reality in deep learning our parameters $b, W$ live in very high dimensions; in these cases, we are much more likely to run into saddle points as opposed to local optimum. What is a problem in higher dimensions are \emph{plateaus}, and this slows down learning. It is in these cases of plateaus that faster learning algorithms like Adam can really help out.

\subsection{Hyperparameter tuning} Changing a neural network can involve modifying many hyperparameters.
This section is on how to systematically organize your hyperparameter tuning process. The sheer number of hyperparameters
can feel overwhelming: learning rate $\alpha$, momentum term $\beta$, or even additional parameters $\beta_1, \beta_2$ and $\epsilon$ 
if using Adam. Perhaps we additionally need to pick the number of layers, the number of hidden units, whether to use a learning
rate decay, and additionally a mini-batch size.

\paragraph{Random Search}
It turns out that of the many hyperparameters, only a few consequentially matter. Perhaps $\alpha$, 
the learning rate $\alpha$, is the 
\emph{most} important. Momentum terms and mini-batch size are also often times worthwhile to consider next, 
followed by the number of 
hidden units. Lastly, a standard recommendation may be to finally 
consider the number of layers and whether to use learning rate decay. 
Whereas it used to 
be common practice to conduct a \emph{grid-search}, it is now recommended to 
search over the same space
\emph{at random}. If we were going to sample $k$ points on a lattice before, we can still sample the same 
number of points at 
random, however by sampling at random we allow ourselves to try more \emph{unique} values of 
each hyperparameter, which is 
crucial since we don't know in advance which are important for a given task. For more details see
\href{http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf}{Bergstra and Bengio '12}.

\begin{minipage}{1.0\textwidth} 
  \begin{multicols}{2} 
    \begin{tikzpicture}[scale=0.75]
      \draw[step=1cm,gray, very thin] (0, 0) grid (5, 5);
      \node[rotate=90] at (-0.6, 2.5) {hyperparameter 1};
      \node            at (2,5.6) {hyperparameter 2};
      \foreach \x in {1, ..., 4} {
        \foreach \y in {1, ..., 4} {
          \node[circle, fill, red, scale=1/2] at (\x, \y) {};
        }
      }
    \end{tikzpicture} 
\vfill\null \columnbreak
\begin{tikzpicture}[     
  declare function={a(\x)=1.7;},     
  declare function={b(\x)=0.1;} ] 
  \begin{axis}[     
    domain=0:2,     
    axis lines=middle,     
    axis equal image,     
    xtick=\empty, 
    ytick=\empty,     
    enlargelimits=true,     
    clip mode=individual, 
    clip=false 
    ] 
    \addplot [red, only marks, mark=*, samples=16, mark size=1]     {0.5*(a(x)+b(x)) + 0.5*rand*(a(x)-b(x))}; 
  \end{axis} 
  \node[rotate=90] at (-0.2, 2.5) {hyperparameter 1};
  \node            at (3.5,-.4)   {hyperparameter 2};
\end{tikzpicture}
\end{multicols} 
\end{minipage}

In practice, we search over more than two (hyperparameters) dimensions; if we search over a grid we suffer from
 the curse of dimensionality.

\paragraph{Coarse-to-fine sampling schemes} Andrew also recommends using a coarse-to-fine sampling schema.
To do this, first apply random sampling to a larger region, and then once a sub-region has been identified
which corresponds to low values of the cost function we can focus more compute resources on a smaller \color{blue}{area}\color{black}.

\begin{figure}[h]
\centering
\begin{tikzpicture}[     
  declare function={a(\x)=1.7;},     
  declare function={b(\x)=0.1;},
  declare function={c(\x)=0.4;},
  declare function={d(\x)=0.7;}
] 
\pgfmathsetseed{4}
  \begin{axis}[     
    domain=0:2,     
    axis lines=middle,     
    axis equal image,     
    xtick=\empty, 
    ytick=\empty,     
    enlargelimits=true,     
    clip mode=individual, 
    clip=false 
    ] 
    \addplot [red, only marks, mark=*, samples=16, mark size=1]     {0.5*(a(x)+b(x)) + 0.5*rand*(a(x)-b(x))}; 
    \addplot [domain=1:1.5, blue, only marks, mark=+, samples=25, mark size=1]     {0.5*(c(x)+d(x)) + 0.5*rand*(c(x)-d(x))}; 
  \end{axis} 
  \node[rotate=90] at (-0.2, 2.5) {hyperparameter 1};
  \node            at (3.5,-.4)   {hyperparameter 2};
  \draw[blue,dashed] (3.25, .75) -- (5,.75) -- (5, 1.75) -- (3.25, 1.75) -- (3.25, .75);
\end{tikzpicture}
\end{figure}

\subsection{Using an appropriate scale to pick hyperparameters} Sampling at random doesn't in fact mean
sampling \emph{uniformly} at random over the range of possible values, 
but instead it's important to carefully select the \emph{scale} over which we search. If it's the case that our search space (in a single dimension) is small, it is acceptable to sample uniformly at random or even perform a grid search: e.g. if we know that the number of layers $L$ should be in the set $\{2, 3, 4\}$, or if we believe the right number of hidden neurons to be between 50 and 100. However, consider a learning rate parameter $\alpha$: we may believe its optimal value to lie somewhere in the interval $(0.001, 1)$. If we sample uniformly at random, we spend 90\% of our resources searching the interval $[0.1, 1)$ and only 10\% searching the smaller end of the interval. This could be problematic. Instead, we may search for (a learning rate) parameters on a \emph{log} scale which in our example might look like $\{0.0001, 0.001, 0.01, 0.1, 1\}$.

\paragraph{Sampling on a log-scale}
How do we implement sampling on a log-scale? It's simple, take the two endpoints of the interval 
we wish to sample from and take the logarithm of each term. The resulting pair of real numbers gives another interval
which we can sample over uniformly at random. Then, after sampling we can exponentiate to get back to our original scale. E.g. $\log_{10}(0.0001) = -4$ and $\log_{10}(1) = 0$. So, we sample from $[-4, 0]$ uniformly at random, and then take these values and apply the transform $f(x) = 10^x$ to recover our desired scale.

\paragraph{Momentum and log scale sampling} Suppose we believe the best value of $\beta$ to lie in the interval
$(0.9, 0.999)$. Realize that this corresponds to averaging over the last 10 or 1,000 values. Therefore, it doesn't make sense to sample uniformly at random from this interval.

\paragraph{Tips and tricks for organizing hyperparameter searches} Intuitions do get stale, so we must 
re-evaluate them occasionally. There are two main methods for improving our models. The first is to ``babysit''
a single model, e.g. we watch the loss function day by day and with each passing day we make a subtle change to our
modeling architecture. On the other hand, we could train many models in parallel and overlay their loss functions
in different colors, choosing the best accordingly. If you have enough compute to train models in parallel, take the ``Caviar'' approach whereby we train many models and choose the best according to a development set. If we don't have a lot of compute, we can babysit the model like a Panda with their offspring.

\subsection{Batch Normalization}
Batch normalization is an algorithm created by Sergey Ioffe and Christian Szegedy, which is designed to make the
hyperparameter search problem a bit simpler whilst also making your network more robust to a \emph{wider range} 
of hyperparameter values. Recall that when training
a model like Logistic regression, we first normalized our input features such that a loss function 
with elongated contours is transformed to one more round and bowl-like which is easier for gradient descent to
optimize over.

\begin{minipage}{1.0\textwidth} 
\begin{multicols}{3}
\begin{tikzpicture}
  \node (x1) at (0, 1) {$x_1$};
  \node (x2) at (0,.5) {$x_2$};
  \node (x3) at (0, 0) {$x_3$};
  \draw[->] (x1) to (1, .6);
  \draw[->] (x2) to (1, .5);
  \draw[->] (x3) to (1, .4);
  \node[draw=black,circle] (z) at (1.25, .5) {};
  \node (params) at (1, .9) {$w,b$};
  \draw[->] (1.5,.5) to (1.8, .5);
  \node at (2, .5) {$\hat y$};
\end{tikzpicture}
\columnbreak
\begin{align*}   \mu &\gets \frac{1}{m} \sum_{i=1}^m x^{(i)} \\
  x &\gets x - \mu \\
  \sigma^2 &\gets \frac{1}{m} \sum_{i=1}^m (x^{(i)})^2 \hspace{10pt} {\footnotesize \texttt{element-wise squaring}} \\
  x &\gets \frac{x}{\sigma^2} \end{align*}
\vfill\null \columnbreak
\begin{tikzpicture}
  \draw (0,1) ellipse (3cm and .75cm); 
  \draw (0,1) ellipse (2cm and .625cm);     
  \draw (0,1) ellipse (1.25cm and .5cm); 
  \draw (0,1) ellipse (.75cm and .375cm);
  \draw[->] (0,0) to (0, -.35);
  \draw (0,-1.5) circle (1cm);
  \draw (0,-1.5) circle (.5cm);
  \draw (0,-1.5) circle (.25cm);
  \draw (0,-1.5) circle (.125cm);
\end{tikzpicture}
\end{multicols} 
\end{minipage}

Batch normalization is the idea of \emph{normalizing} outputs from hidden layers; there is some debate
in the deep learning community about whether to apply the normalization before or after the activation function (i.e.
do we apply to $z_i$ or $a_i$), but normalizing $z_i$'s is done more often. The core question at heart is: can we normalize $a^{[i]}$ (or $z^{[i]}$) in such a way that we train $W^{[i]}$ and $b^{[i]}$ faster.

\subsubsection{Normalizing activations in a network} 
To implement batch normalization, suppose that we are given some
hidden unit values $z^{(1)}, \ldots, z^{(m)}$, where we have omitted the superscript square bracket $[\ell]$ indicating
which layer we are concerned with since this example just focuses on an arbitrary layer; if we wanted to be rigorous we could write this out with full notation as $z^{[\ell](1)}, \ldots, z^{[\ell](m)}$. In any case, we compute a mean and variance for the inputs to the activation function for the layer and we normalize:
\begin{align*}   \mu = \frac{1}{m} \sum_{i=1}^{m} z^{(i)}, \hspace{15pt}
  \sigma^2 = \frac{1}{m} \sum_{i=1}^m \left(z_i - \mu\right)^2, \hspace{15pt}
  z^{(i)}_{\texttt{norm}} = \frac{z^{(i)} - \mu}{\sqrt{\sigma^2 + \epsilon}} \end{align*}
where we have taken care to inflate our denominator by some fixed $\epsilon$ in the event that an estimate for
$\sigma^2$ turns out identically zero. After this transformation, each component of our vector $z$ will have 
zero mean and unit variance. However, we don't \emph{always} want the hidden units to follow a distribution
with these first two moments; instead, we compute
\[
\tilde{z^{(i)}} = \gamma z^{(i)}_{\texttt{norm}} + \beta
\]
where $\gamma$ and $\beta$ are learnable parameters of the model. I.e. for whatever optimization algorithm 
we employ, whether it be gradient descent or RMSprop or Adam, we update $\gamma$ and $\beta$ in the same way that
we update $W$'s and $b$'s. Notice that we could theoretically recover our original input to $a^{[\ell]}$ if we set 
$\gamma$ to $\sqrt{\sigma^2+\epsilon}$ and $\beta$ to $\mu$. However, we're also free to learn
parameters $\gamma$ and $\beta$ such that $\tilde{z^{(i)}}$ has an arbitrary mean and variance. 

\paragraph{Why might we prefer
the first two moments of $\tilde{z^{(i)}}$ to be different from zero and one?} Well, suppose we're using 
a sigmoid activation function. Then, in order to take advantage of the non-linearities of the function we'd
like \emph{not} to squish our input values to a range concentrated around the origin since this is where the 
activation function behaves linearly.

\begin{figure}[h]
\centering
\begin{tikzpicture}
  \draw[->] (-3,0) -- (3,0) node[right] {};   
  \draw[->] (0, 0) -- (0,1) node[above] {};
  \draw[domain=-3:3,smooth,variable=\x,orange] plot ({\x},{1 / (1 + (exp(-\x)))}); 
  \draw[domain=-1:1,smooth,variable=\x,purple] plot ({\x},{1/4*1 / sqrt(2*pi*1/10) * exp(-(\x)^2/(2*1/10))}); 
  \draw[domain=-1:1,smooth,variable=\x,blue,dashed] plot ({\x},{1/4 * \x + 1/2}); 
\end{tikzpicture}
\end{figure}
Instead, perhaps we might like to enforce a larger variance such that we can explore areas of the activation function which are non-linear. In any case, the learning algorithm is free to set $\gamma$ and $\beta$ in such a way that 
it converges more quickly.

\subsubsection{Fitting batch-norm into a neural network} 
Recall that each neuron in a network computes two quantities:
a linear combination using $W, b$ to yield $z$ which is then used as input to an activation function.

\begin{figure}[h]
\centering
\begin{tikzpicture}[darkstyle/.style={circle,draw,fill=gray!40,minimum size=20}]
  \node (x1) at (-1.3, 1.5) {$x_1$};   \node (x2) at (-1.3, 0) {$x_2$};   
  \foreach \x in {0, 1} {
    \foreach \y in {0,...,1} {
      \node[draw,circle,scale=2]  (\x\y) at (1.5*\x,1.5*\y) {};
      \draw (1.5*\x,1.5*\y-.3) -- (1.5*\x,1.5*\y+.3);
    }
  }
  \node[scale=0.5] at (-.2, 1.5) {$z^{[1]}_1$};
  \node[scale=0.5] at (0.2, 1.5) {$a^{[1]}_1$};
  \node[scale=0.5] at (-.2, 0) {$z^{[1]}_2$};
  \node[scale=0.5] at (0.2, 0) {$a^{[1]}_2$};
  \node[scale=0.5] at (1.5-.2, 1.5) {$z^{[2]}_1$};
  \node[scale=0.5] at (1.5+0.2, 1.5) {$a^{[2]}_1$};
  \node[scale=0.5] at (1.5-.2, 0) {$z^{[2]}_2$};
  \node[scale=0.5] at (1.5+0.2, 0) {$a^{[2]}_2$};
  \draw[->] (x1) to (00);
  \draw[->] (x1) to (01);
  \draw[->] (x2) to (00);
  \draw[->] (x2) to (01);
  \draw[->] (00) to (10);
  \draw[->] (00) to (11);
  \draw[->] (01) to (10);
  \draw[->] (01) to (11);
  \node[draw,circle] (outputneuron) at (3, .75) {}; 
  \draw[->] (10) to (outputneuron);
  \draw[->] (11) to (outputneuron);
  \node (yhat) at (4, .75) {$\hat y$};
  \draw[->] (outputneuron) to (yhat);
\end{tikzpicture}
\end{figure}

Our computation graph for our input now through the first layer now looks like:

\begin{figure}[h]
\centering
\begin{tikzpicture}   \node (x) at (0,0) {$x$};
  \node[right = 2cm of x] (z1) {$z^{[1]}$};
  \node[right = 2cm of z1] (zt) {$\tilde z^{[1]}$};
  \node[right = 1cm of zt] (a1) {$a^{[1]} = g^{[1]}(\tilde z^{[1]})$};
  \node[above right = 0.125cm of x] (w) {$W^{[1]}, b^{[1]}$};
  \node[above right = 0.125cm of z1] (beta) {$\gamma^{[1]}, \beta^{[1]}$};
  \node[below right = 0.075cm of z1] (bn)   {{\small\texttt{batch-norm}}};
  \draw[->] (x) to (z1);
  \draw[->] (z1) to (zt);
  \draw[->] (zt) to (a1);
\end{tikzpicture}
\end{figure}
We then feed-forward the outputs of $a^{[1]}$ into the inputs of $z^{[2]}$, which is governed by 
learnable parameters $W^{[2]}, b^{[2]}$, after which we perform another batch-norm governed by learnable
parameters $\gamma^{[2]}$ and $\beta^{[2]}$. This normalized $\tilde z^{[2]}$ is used to compute $a^{[2]}$, and we 
continue in this fashion through the remainder of the network if desired.

\paragraph{Parameters for a batch-norm network}
Note that the parameters of our network used to only contain $W^{[1]}, b^{[1]}, \ldots, W^{[L]}, b^{[L]}$ but now 
additionally contain $\gamma^{[1]}, \beta^{[1]}, \ldots, \gamma^{[L]}, \beta^{[L]}$. To be clear, these $\beta$'s are 
completely different from the terms used in momentum, Adam, and RMSProp.

\paragraph{Working with mini-batches} So far, we've pretended that we are using vanilla gradient descent, 
processing our entire dataset each iteration. In practice, batch-norm is often used alongside mini-batch gradient
descent, for example. In this case, on our first iteration of mini-batch gradient descent we use
$X^{\{1\}}$ as input and feed-forward through our network (applying batch-norm at each layer if desired). Then, on the
second iteration of mini-batch gradient descent we use $X^{\{2\}}$ as input, where now the batch-norm computations depend only on this input (and its intermediary transforms) but no longer on $X^{\{1\}}$. I.e. batch-norm at each iteration of mini-batch gradient descent should only depend on the current mini-batch of data and none others.

\paragraph{Parameters of a network which utilizes batch-norm} We've so far acted as though our parameters for a
layer $\ell$ are $W^{[\ell]}$, $b^{[\ell]}$, $\gamma^{[\ell]}$, and $\beta^{[\ell]}$. However, realize that 
$z^{[\ell]} = W^{[\ell]} a^{[\ell-1]} + b^{[\ell]}$, and that the subsequent step in batch-norm is to subtract out a mean and then later perform a translation via $\beta^{[\ell]}$; adding this constant $b^{[\ell]}$ is nullpotent since we later subtract out a common value from all elements in $z^{[\ell]}$. Therefore, if using batch-norm we can eliminate using $b^{[\ell]}$ (or we can think of permanently setting it to zero). Then, our transformations become 
$z^{[\ell]} = W^{[\ell]} a^{[\ell-1]}$ and then we can compute $z^{[\ell]}_{\texttt{norm}}$, and then we compute 
$\tilde z^{[\ell]} = \gamma^{[\ell]} z^{[\ell]}_{\texttt{norm}} + \beta^{[\ell]}$. In this case, $\beta^{[\ell]}$ controls our 
bias term that gets added back in. Note that $\gamma^{[\ell]}, \beta^{[\ell]} \in (n^{[\ell]}, 1)$.

\begin{algorithm}   \caption{Batch-norm}
  \For{$t=1, \ldots, \texttt{numMiniBatches}$} {
    Compute forward prop on $X^{\{t\}}$; in each hidden layer use Batch-norm to replace $z^{[\ell]}$ with $\tilde z^{[\ell]}$. \\
    Use backpropagation to compute $\d W^{[\ell]}$, $\d \beta^{[\ell]}$, and $\d \gamma^{[\ell]}$. \\
    Update parameters $W^{[\ell]}$, $\beta^{[\ell]}$, and $\gamma^{[\ell]}$ using {\small gradient descent (with momentum) / RMSProp / Adam}.
  } \end{algorithm}

\subsubsection{Why does batch-norm work?}
One of the primary benefits of batch-norm is that it makes the weights in the \emph{later} layers of the network
more robust to varying input values in \emph{earlier} layers of the network.
To see this, consider an example whereby we train a computer-vision classifier to predict whether an image contains
a cat. However, we only trained our network on images with \emph{black} cats, and in a new set of unseen test cases
we are given images with cats of varying colors. Even though the true decision boundary may be the same, we wouldn't expect our learning algorithm to discover this if it were only trained on the input shown on the left below. Correspondingly, it may not perform so well on the perturbed data drawn on the right below. The notion of data distribution changing is given the name \emph{covariate shift}.

\begin{minipage}{1.0\textwidth} 
\begin{multicols}{3}
\begin{tikzpicture}
  \node (x1) at (0, 1) {$x_1$};
  \node (x2) at (0,.5) {$x_2$};
  \node (x3) at (0, 0) {$x_3$};
  \draw[->] (x1) to (1, .6);
  \draw[->] (x2) to (1, .5);
  \draw[->] (x3) to (1, .4);
  \node[draw=black,circle] (z) at (1.25, .5) {};  % \node (params) at (1, .9) {$w,b$};
  \draw[->] (1.5,.5) to (1.8, .5);
  \node at (2, .5) {$\hat y$};
\end{tikzpicture}
\vfill\null \columnbreak
\begin{tikzpicture} \draw[->] (-1,0) -- (3,0) node[right] {};   
\draw[->] (0,0) -- (0,1) node[above]  {};
\draw[domain=-0:2,smooth,variable=\x,green] plot ({\x},{-(\x-1)^2 + 1});   
\node[draw=red, rectangle,scale=1/2] at (1/2, 1/2) {};
\node[draw=red, rectangle,scale=1/2] at (2/3, 3/4) {};
\node[draw=red, rectangle,scale=1/2] at (1/3, 1/4) {};
\node[draw=red, rectangle,scale=1/2] at (3/4, 1/4) {};
\node[draw=blue, circle,scale=1/2] at (1/2, 3/2) {};
\node[draw=blue, circle,scale=1/2] at (1/3, 5/4) {};
\node[draw=blue, circle,scale=1/2] at (1/2, 1) {};
\end{tikzpicture}
\vfill\null \columnbreak
\begin{tikzpicture} 
\draw[->] (-1,0) -- (3,0) node[right] {};   
\draw[->] (0,0) -- (0,1) node[above]  {};
\draw[domain=-0:2,smooth,variable=\x,green] plot ({\x},{-(\x-1)^2 + 1});   
\node[draw=red, rectangle,scale=1/2] at (1/2+1/5, 1/2) {};
\node[draw=red, rectangle,scale=1/2] at (2/3+1/2, 3/4) {};
\node[draw=red, rectangle,scale=1/2] at (1/3+1, 1/4-1/5) {};
\node[draw=red, rectangle,scale=1/2] at (3/4+1/3, 1/4) {};
\node[draw=blue, circle,scale=1/2] at (1, 3/2) {};
\node[draw=blue, circle,scale=1/2] at (4/3, 5/4) {};
\node[draw=blue, circle,scale=1/2] at (2, 1) {};
\end{tikzpicture}
\end{multicols}
\end{minipage}

The idea behind covariate shift is that if we learned some $X \leadsto Y$ mapping, then if the distribution of 
$X$ changes we may need to retrain our learning algorithm. This remains true even if the ground truth function mapping $X \leadsto Y$ has remained static, as in our example above. Note that if the ground truth function \emph{also}
shifts, then the problem of covariate shift becomes even more acute.

\paragraph{Understanding batch-norm and how it enables learning between layers to be decoupled}
Consider a multiple-layer neural network. The job of an intermediary layer is to take the output activations from the layer prior and learn a mapping (through tuning weights $W, b$) from these inputs to $\hat y$. But because the output activations from the layer prior also depend on learned weights, the distribution can change quite drastically during training. What batch-norm does is effectively mitigate the amount of covariate shift from previous layers by 
ensuring that the input to the current layer has a fixed mean and variance. By limiting how updates in the network to earlier layers can affect the distribution of inputs to later layers, we cause the input values to later nodes of the network to become more stable and this really improves learning. So, even \emph{if} our input data does shift distribution, the learned parameters $\gamma, \beta$ in earlier layer will ensure that the values input to latter layers remains more stable. From the perspective of the latter layers, the inputs haven't shifted \emph{as} much. 
Put differently, we \emph{decouple} learning between layers, allowing each to learn by itself. 

\paragraph{Unintended side-effect: regularization}
Another interesting note is that batch-norm can be viewed as a form of regularization. To see this, first understand
that in each mini-batch $X^{\{t\}}$ of data we process, the values $z^{[\ell]}$ are re-scaled based only on $X^{\{t\}}$
 which may only consist of 64, 128, or 256 examples. So, our calculations for mean and variances are a little 
bit noisy as a result, and this in turn means that our scaling process of going from $z^{[\ell]}$ to $\tilde z^{[\ell]}$ is also a bit noisy. By adding some noise to inputs of hidden units, its forcing downstream hidden units not to rely too much on any one hidden unit. Similar to dropout, it adds noise to the hidden layers and therefore has a slight regularization effect. Note that if we choose a larger mini-batch size, our estimates for $\mu$ and $\sigma^2$ become more stable and the effect of regularization is reduced. This all said, the primary intent of batch-norm is to 
speed up training, not to encourage regularization.

\subsubsection{Batch-norm at Test time} Batch-norm processes our data one mini-batch at a time. But, at test time
we might need to process examples one at a time. How can we adapt our network to do that? Recall that during training,
we use the following equations to implement batch-norm; we use $m$ to denote the number of examples in the mini-batch (\emph{not} the whole training set).
\begin{align}   \mu &= \frac{1}{m}\sum_{i=1}^m z^{(i)} \nonumber \\
  \sigma^2 &= \frac{1}{m} \sum_{i=1}^m \left(z^{(i)} - \mu\right)^2 \nonumber \\
  \label{eq: zinorm}
  z^{(i)}_{\texttt{norm}} &= \frac{z^{(i)} - \mu}{\sqrt{\sigma^2 + \epsilon}} \\
  \tilde z^{(i)} &= \gamma z^{(i)}_{\texttt{norm}} + \beta \nonumber \end{align}

Notice that $\mu, \sigma^2$ are both computed on the entire mini-batch. But at test time, we may not have a mini-batch
of size 64, 128, or 256 examples to process at a time. What's actually done? We come up with separate estimates of
$\mu, \sigma^2$ where we typically estimate these values using exponentially weighted averages across mini-batches.
To make this concrete, fix attention to some layer $\ell$. As we train our network on different mini-batches $X^{\{1\}}, X^{\{2\}}, X^{\{3\}}, \ldots$, we accumulate different estimates for $\mu^{\{1\}[\ell]}, \mu^{\{2\}[\ell]}, \mu^{\{3\}[\ell]}, \ldots$. The exponentially weighted averages across mini-batches informs us of our estimate for $\mu$. Similarly for $\sigma^2$. So, as we train our neural network we accumulate a running average of these $\mu, \sigma^2$ parameters that we plan to use at test time. At test time, we modify \ref{eq: zinorm} to use our (exponentially weighted) estimates for 
$\mu, \sigma^2$. We still apply the learned parameters $\gamma, \beta$ learned at train time to re-scale our $z^{(i)}_{\texttt{norm}}$. Lastly, we remark that there are many ways we could go about estimating $mu, \sigma^2$ to apply at test time, but that the performance of a network does not typically hinge on these hyperparameters too much.

\subsection{Multi-class classification} 
\paragraph{Generalization of Logistic regression: Softmax regression}
So far, the examples we've used for classification have been 
\emph{binary}, where there are only two possible labels. For instances where there are multiple possible classes,
there is a generalization of logistic regression called \href{https://en.wikipedia.org/wiki/Softmax_function}{Softmax} regression. Denote by $C$ the number of classes we are trying to categorize our inputs into. We index into our classes from $\{0, 1, \ldots, C-1\}$. The goal is to build a new neural network where the output layer has $C$ units. We'd like for the output units to inform us of the probability of belonging to the corresponding class. To be explicit, in Python notation our output vector is now $(C, 1)$ dimensional.

\paragraph{Multiple neurons in output layer}
In the final layer of our network with $C$ nodes, each node still consists of two parts: a linear mapping and a non-linear mapping. We still compute $z^{[L]} = W^{[L]} a^{[L-1]} + b^{[L]}$, and once we have this we can now feed it into a Softmax activation function. The first step is to compute a temporary variable $\texttt{tmp} = e^{z^{[L]}}$ where the exponential function is applied element-wise. Then, the output for the final layer is given by
\[
a^{[L]} = \frac{e^{z^{[L]}}}{\sum_{j=1}^C \texttt{tmp}_j}.
\]
\paragraph{Output layer yields a probability distribution}
In other words, each element $a_i^{[L]} = \frac{\texttt{tmp}_i}{\sum_{j=1}^C t_j}$ is an element of a probability 
distribution since we've (i) mapped the real number line to the positive half-space and then (ii) 
performed normalization by a sum such that the elements add to unit value. The unusual thing about the 
Softmax activation function is that it takes as input a real vector and outputs a real vector, as opposed to our 
previous activation functions which are scalar valued. One important aspect of a Softmax classifier is 
that the decision boundary between any two classes is linear, 
so if we don't use any non-linear transformations in our hidden layers then we will end up with a linear decision boundary between classes.

\subsubsection{Training a Softmax classifier} Notice that the Softmax function maps the largest input value to the largest probability element. This is analagous to a hardmax function, which for a given vector $(x_1, \ldots, x_n)$,
emits a binary vector of equal length consisting of all zeros except for the position corresponding to the maximum input, which consists of a unit value. E.g. hardmax would map $\begin{bmatrix}   12 & 4 & -3 \end{bmatrix}$ to $\begin{bmatrix}   1 & 0 & 0 \end{bmatrix}$.

\paragraph{Loss and cost function} Let's consider our loss function for multi-class classification.
\begin{equation}
\label{eq: multiclassloss}
\mathcal L (\hat y, y) = -\sum_{j=1}^C y_j \log \hat y_j.
\end{equation}
We can see why this makes sense on a toy example. Suppose we have our multi-class output layer ground-truth $y = \begin{bmatrix}
0 & 1 & 0 & 0 \end{bmatrix}$, i.e. this is an example of the second class. Then notice that $y_1 = y_3 = y_4 = 0$, and our 
loss function \ref{eq: multiclassloss} reduces to $-y_2 \log \hat y_2 = -\log \hat y_2$. Since we wish to minimize our
loss function, this is equivalent to making $\hat y_2$ really large, i.e. we want to emit a large probability for this
label. What about our \emph{cost} function? It's what you'd expect:
\[
J(W^{[1]}, b^{[1]}, \ldots) = \frac{1}{m} \sum_{i=1}^m \mathcal L(\hat y^{(i)}, y^{(i)}).
\]

\paragraph{Dimensions of output}
Let's clarify our dimensions. We have $Y = \begin{bmatrix}   y^{(1)} & y^{(2)} & \ldots & y^{(m)} \end{bmatrix}$ where each $y^{(i)}$ is $(C, 1)$ dimensional. So, $Y, \hat Y \in (C, m)$.

\paragraph{Backpropagation and gradient descent with Softmax} The key equation is that $\frac{\partial J}{\partial z^{[L]}} = \d z^{[L]} = \hat y - y$ which is a $(C, 1)$ dimensional vector.

\section{Structuring Machine Learning Projects}
\subsection{Introduction to ML Strategy}
\paragraph{Why ML Strategy?} Suppose you're working on a cat classifier, which in spite of having 90\% accuracy
is not good enough for your application. You start to brainstorm ideas: collect more data, collect more diverse 
data, train the algorithm longer, or try a different algorithm, or we could also think about network sizes (either smaller or bigger), trying dropout, adding $\ell_2$ regularization, rewiring the network architecture including
activation functions and the \# of hidden units. What if we spend 6 months acquiring more training data, only to realize that it barely helps our model? Wouldn't it be nice if we could know which techniques are worthwhile and which
we can safely discard? This course is about strategy, motivated by practical learnings from Andrew Ng. 

\subsubsection{Orthogonalization} There are many hyperparameters we can tune.Orthogonalization is the notion of being clear-eyed about what exactly we're tuning in order to have a particular desired effect. Consider as an example an old-school television set, with many knobs. One enlarges the picture vertically, another horizontally, another controls 
how trapezoidal the image appears, whilst others allow vertical and horizontal translation as well as rotation. It's an important UI/UX feature that each knob controls an interpretable function. Imagine how hard it would be to tune a
television where the first knob controls a linear combination of the adjusters we listed above (e.g.
$0.1 \times \texttt{height} + 0.3 \times \texttt{trapezoidal} + \ldots$). 

In the context of the television designers,
orthogonalization refers to designing each knob to achieve a single outcome. This makes it easier to tune the television and center the picture. Another example could be in the context of a car: we have a steering wheel to control left-and-right direction, as well as a gas and brake pedal to control speed. Theoretically, if we had a joystick that controlled $0.3 \times \texttt{steering\_angle} - 0.8 \times \texttt{speed}$ and another joystick which controlled 
$2 \times \texttt{steering angle} + 0.9 \times \texttt{speed}$, this would suffice to control the car however we'd like. But it's much harder to control than if we had separate controls for each function. 

\paragraph{We tune knobs to satisfy the following four conditions}
For a supervised learning system, we'd like to tune knobs so that the following hold true:
performance is acceptable on the training set (perhaps relative to human level performance), 
the development set, the test set, and finally we'd like for our cost
function when applied to the test set to correlate well with real world performance.
\begin{enumerate}   \item If we're not fitting to the training set well on the cost function, we might train a bigger network, or a better optimization algorithm like Adam.
  \item If we're not fitting well on the development set but it is on the training set, we might consider a set of knobs around regularization or getting a larger training set.
  \item If we're not fitting well on the test set but we are on the development set, it probably means the development set was too small (i.e. we've overfitted to it) and so we need to acquire a larger development dataset.
  \item If we're not performing well in the real world but we are on the test set, then we need to go back and change either the development set or the cost function. \end{enumerate}

One example of a knob that is \emph{not} orthogonalized is \emph{early-stopping}: this is an operation that simultaneously affects performance on training set (stopping early you fit the training set less well) but its often intended to improve performance on the development set.

\subsection{Setting up your goal}
\paragraph{Single (real) number evaluation metric} Progress will be much faster if you have a single real number evaluation metric which lets you tell if your new idea is making an improvement (or not). As a motivating example, consider
a cat classifier; it's reasonable to examine both precision (the percentage of predictions labeled as cats that actually are cats) and recall (the percentage of the cat population that was correctly classified as a cat). There is often
a tradeoff between precision and recall, and it turns out we'd like to have both: when the classifier says something
is a cat, we'd like there to be a high chance of this being true, but similarly for all the images of cats we'd want
our classifier to correctly label most of them as cats as well. Having
two real numbers makes it tough to evaluate many different models against each other. In machine learning, it's traditional to combine both of these metrics into an \href{https://en.wikipedia.org/wiki/F1_score}{F1 score}, which is the \href{https://en.wikipedia.org/wiki/Harmonic_mean}{harmonic mean} of \href{https://en.wikipedia.org/wiki/Precision_and_recall}{precision and recall}.
\[
F_1 = \frac{2}{\texttt{recall}^{-1} + \texttt{precision}^{-1}} = 2 \cdot \frac{\texttt{precision} \cdot \texttt{recall}}{\texttt{precision} + \texttt{recall}}.
\]
 
\paragraph{Satisficing and Optimizing Metrics} 
What if we're considering not only the accuracy of a classifier, but also its cost in compute time? It might seem artificial to come up with a linear combination of these metrics in order to derive a single real number, e.g. \texttt{accuracy - 0.5 x runningTime}. Instead, we might decide that we'd like to \emph{maximize} accuracy \emph{subject to}
the latency of our algorithm being below some threshold. Then, we say that accuracy is an optimizing metric and
 latency is a satisficing metric.

If we have $N$ metrics we really care about, Andrew recommends making one of them an optimizing metric and
the reamining $N-1$ satisficing.

\paragraph{Train/dev/test distributions} Note that the development set is 
sometimes called the cross-validation dataset in other literatures. The core workflow in machine learning is
\begin{itemize} \item Try a lot of ideas by training different models on the \emph{train}ing set.
\item Use the development set to evaluate the different ideas and select one.\footnote{Andrew states that it's permissible to keep innovating by improving development set performance.}
\item Evaluate the efficacy of your approach via the test set. \end{itemize}

It's critical that the development and test sets come from the same distribution. As an example, consider training a classifier to predict the likelihood of a loan application being processed on data restricted to \emph{medium} income
postal codes; such a classifier may not perform well at all when evaluated on data drawn from \emph{low} income
postal codes.

\paragraph{Size of development and test sets} We've emphasized that development and test sets should come from the same distribution. But how large should they be? In the past when datasets were smaller, using a rule of thumb like
allocating 60/20/20 for train/dev/test was reasonable. But modern datasets are so large that it might be quite reasonable to allocate 98\% for training, and another 1\% for development and test each. Since many algorithms have an appetite for data, it's reasonable to allocate more toward training. Remember as well that the purpose of the test set is simply to get an unbiased estimate of error; unless you need to have a very accurate measurement of how your final system is performing, you might not need millions (and millions) of examples but instead maybe 10-100k. It's worth noting that the size of the development set must be large enough that we overfit to it.

\paragraph{When to change dev/test sets and metrics} Sometimes, partway through a project you realize you may have
set up your dev/test sets incorrectly; in this case, you should move your target accordingly. Consider two cat classifiers where one has slightly lower test error than another but allows pornographic images to also be shown to users.
This would be totally unacceptable, and so in light of this we might want to change either our dev/test sets or our
evaluation criterion. Suppose we define error as $\frac{1}{m_{\texttt{dev}}} \sum_{i=1}^{m_{\texttt{dev}}} \mathbbm 1_{\{\hat y^{(i)} \neq y^{(i)}\}}$, then we may wish to modify our criterion for error to instead be something like
\begin{align*}   & \hspace{10pt} \frac{1}{\sum_{i=1}^{m_{\texttt{dev}}} w^{(i)}} \sum_{i=1}^{m_{\texttt{dev}}} \color{orange}{w^{(i)}} \color{black} \mathbbm 1_{\{\hat y^{(i)} \neq y^{(i)}\}}, \textrm{ where } \\
  w^{(i)} &= \begin{cases}     1 & \textrm{if } x^{(i)} \textrm{ is non-pornographic} \\
    100 & \textrm{if } x^{(i)} \textrm{ is pornographic}   \end{cases} \end{align*} where we've added weights to properly penalize presenting a pornographic 
image and we've also updated our loss function's normalization constant such that error lies in the unit interval.
Recall the concept of orthogonalization: we (i) define a metric that captures we'd like to achieve and then (ii) 
worry about how to optimize over that metric. One example of when to change dev/test sets is if we're training an image classifier on professional photographs but users are interfacing with our model by providing as inputs more blurry
and less focused photos; we might consider switching dev/test sets in this case.

\subsection{Comparing to human-level performance}
A lot more machine learning teams are starting to compare their performance to that of what humans can achieve.
There are two main reasons: (i) advances in deep learning mean that it's actually feasible for an algorithm to
achieve human level performance, and (ii) the workflow of designing and building a machine learning system is more 
efficient when you're trying to emulate something a human can also do. In these settings, it becomes natural to compare the performance of an algorithm with a human (figure \ref{fig: progress-over-time}).
\begin{figure}[h]
  \centering   \begin{tikzpicture}[scale=1.75]     \draw[->] (0,0) -- (3,0) node[right] {\textrm{time}};   
    \draw[->] (0,0) -- (0,1.25) node[above] {\textrm{accuracy}};
    \draw[domain=0:3,smooth,variable=\x,orange] plot ({\x},{(tanh(\x))});     \draw[domain=0:3.25,smooth,variable=\x,purple, dashed] plot ({\x},{1}); 
    \draw[domain=0:3.25,smooth,variable=\x,purple, dashed] plot ({\x},{0.85}); 
    \draw[<-] (3.25, 1) -- (3.5, 1.5);
    \draw[<-] (3.25, 0.85) -- (3.5, 0.85);
    \node at (3.5, 1.5) [right] {\textrm{theoretical optimum level of perf.}};
    \node at (3.5, 0.85) [right] {\textrm{human level performance}};
  \end{tikzpicture}
  \label{fig: progress-over-time}
  \caption{A typical plot of progress on a problem within a machine learning team. Progress is rapid until
    human level performance is surpassed, at which point progress may still continue but at a slower rate.
    Eventually and in an ideal world, performance of an algorithm approaches the theoretical upper bound on 
    performance which is dictated by the Bayes error rate.} \end{figure}

It's important to emphasize that no matter how hard you work on a problem, you can never surpass \href{https://en.wikipedia.org/wiki/Bayes_error_rate}{Bayes error rate}, and further no human can outperform Bayes error 
rate by definition.\footnote{An algorithm only outperforms the Bayes error rate on a training dataset if its overfitting.}
But it also turns out that progress is quite fast until human level performance is surpassed. Why is this?  One reason
is that for many tasks, human level performance is not that far off from Bayes optimal error, i.e. people are very 
good at various tasks. But a more fundamental reason is that tools that can help improve model accuracy that are available to use for models performing below human level performance become intractable to use when the model has surpassed human level performance. Specifically, so long as your machine learning algorithm is performing worse than what humans, you can higher humans to create labeled data to feed your algorithm. By asking raters to classify inputs that your
algorithm is likely to get wrong, you (or your algorithm) can gain an insight as to where you went wrong; this helps
to improve model performance. Once your algorithm starts outperforming humans, this option is no longer efficacious.

\paragraph{Avoidable bias} Sometimes, understanding how accurately humans can perform a task can inform us of whether
we should focus on reducing bias or variance in our model. For example, suppose we have a task of classifying whether
a cat is in an image. Perhaps a human error rate is 1\%, but our model shows 8\% error on development set and 10\% on
the test test; in this case, we'd like to focus on reducing the bias of our algorithm by training a larger network,
or running our training algorithm longer (the point is to focus on 
things that increase performance on the \emph{training} set). But what if the input images are really blurry and
humans can only achieve a 7.5\% error rate?  In this case we might be wise to focus on reducing the variance of our algorithm, e.g. trying regularization or getting more training data to close the gap between dev/test error. Andrew coins the gap between Bayes error and training error to be \emph{avoidable bias}.
\begin{equation*}   \texttt{AvoidableBias} = \texttt{BayesError} - \texttt{TrainingError}. \end{equation*}

\paragraph{Understanding human-level performance} How can we define the phrase ``human-level performance'' more 
precisely? We'll start with an example of medical image classification: given a radiology image make a diagnosis 
decision; suppose an untrained human achieves 3\% error, a doctor 1\% error, an experience doctor 0.7\% error, and a panel of doctors who discuss the image by committee can earn a 1/2\% error. The question becomes, which benchmark serves as human level performance? We know that Bayes error is less than or equal to 1/2\%, and since we like to proxy Bayes error by human level performance, it makes sense to say that human level performance is 1/2\% in this example. But it also could be the case that for the purpose of deploying a system in practice, that we'd only need to outperform a typical doctor; this could be a very useful result if accomplished and so in some cases it could make sense to argue toward defining human level performance as 1\% in the example above. The point is that we should be clear about what our intentions are when defining human level performance: is it to approximate Bayes error or is it to put a machine learning system into deployment?

So, to summarize, when we consider the bias-variance problem(s) and which to focus on first, it's important to compare
training set error not with 0\% error rate but with Bayes error rate or a proxy thereof, via human-level performance.
Then, once we have an idea of the magnitude of avoidable bias as well as the difference between dev/test error, we can
choose the larger problem to focus our efforts on.

\paragraph{Surpassing human-level performance} What happens if an algorithm outperforms a team of human experts working together, and simultaneously achieves low dev/test error gap? Here, it's not obvious whether to focus on the bias or variance problem, since we have no estimate for Bayes error: we simply know that it's at or below what our model is performing on the development (and test) sets. What are some example problems where ML has significantly surpassed human-level performance? Some instances are in online advertising (estimating likelihood of a person clicking an ad), another is product recommendations, logistics (predicting transit times), or loan approvals. It's noteworthy that all
these datasets are based on structured data, and none of the problems are based on \emph{natural perception}
tasks such as computer vision or natural language processing. Lastly, each of the examples where algorithms surpass human level performance have enjoyed massive amounts of data. 

\paragraph{Two fundamental assumptions of supervised learning}
\begin{enumerate}   \item You can fit the training set pretty well (avoidable bias).
  \item The training set performance generalizes pretty well to the dev/test set. \end{enumerate}
To improve the performance of an ML system, first get a notion of avoidable bias
by looking at the difference between human-level performance and training error.
Then, get a notion of magnitude of the variance problem by examining the difference
between dev/test error. To reduce avoidable bias, tactics like training larger models, training longer, using a better optimization algorithm, or even rewiring the network architecture.  If variance is a problem, you can get more training data (to help generalize to dev/test sets), regularization ($\ell_2$, dropout, data augmentation), or rewiring the network architecture. 

\subsection{Error Analysis} Suppose you are trying to get a learning algorithm to accomplish what a human can,
but your algorithm has not yet achieved human level performance. Then, inspecting mistakes that your algorithm
has made can give insight into what to do next; this process is known as error analysis.

\paragraph{Carrying out error analysis} Suppose you have a hunch that your classifier is making a particular type
of mistake that you can envision fixing with diligent effort. The question becomes is it possible to know in advance
whether it's ``worth it'' to embark on such a project. An error analysis might consist of getting 100 mislabeled
development set examples, and counting how many are attributable to the type of mistake you're interested in.
This yields an upper bound on how much performance could be improved if we were to correctly classify these examples. In just a few minutes, this gives an estimate of whether its worth tackling a particular type of problem.

\begin{table}[h]
\centering \begin{tabular}{c|c c c | c}
Image & Dog & Great Cats & Blurry & Comments \\
\hline
1 & \checkmark & & & Pitbull \\
2 & & & \checkmark & \\
3 & & \checkmark & \checkmark & Rainy day at zoo\\
$\vdots$ & $\vdots$ &$\vdots$ &$\vdots$ & \\
\hline
\% of total \end{tabular}  
\caption{Often, we have multiple ideas for how to improve an algorithm. Andrew recommends making a spreadsheet tracking the
error analysis for each idea in parallel; e.g. the table above.
The conclusion of this process informs us where to direct our efforts, as well as upper bounds for how much we can
expect to improve performance.} 
\end{table}

\paragraph{Cleaning up incorrectly labeled data}
A supervised learning problem comprises of a mapping $X \leadsto Y$.
Suppose we dig through our data and find that some output labels in $Y$ are incorrect.
Is it worth your time to fix some of these labels? Note that we make a semantic distinction here:
an algorithm that doesn't predict the right label is said to \emph{mislabel} an example, but an example
that is incorrectly marked in the (training/dev/test) data itself is said to be an \emph{incorrect} label

If the errors are reasonably random it's not worth worrying about as learning algorithms tend to be robust to random errors.
So long as the total dataset size is large enough, and the actual percentage of incorrectly labeled examples is not \emph{too} high,
it's probably not worth your time to fix the mistakes. If the errors are systematic it's a different story. Andrew recommends proceeding with an error analysis as described above, by adding a column counting the number of incorrect labels among mislabeled predictions, so that we can estimate an upper bound on the benefit we'd get from fixing the labels ourselves.

If you do decide to correct labels in your dev set, you should apply the same procedure to your test set and at the same time. This
is in order to preserve the similarity in distribution between the two datasets. That said, we don't always need to apply the same
procedure to the training data, for several reasons: the training data are much larger and so this can be a much more onerous task, and learning algorithms can handle cases where they are trained on data from a slightly different distribution than what they are evaluated on (so long as dev/test are consistent, our estimates for which algorithm is best and its performance are unbiased).

\paragraph{Build your first system quickly, then iterate} This is a piece of advice from Andrew 
that applies less if (i) you have significant domain experience or (ii) there is a body of academic literature
you can draw upon that addresses the same problem. 
Suppose you're building a speech recognition system: there are many directions you could go in terms of how to prioritize 
improvements. E.g. you would focus on
\begin{itemize}[topsep=2pt,itemsep=0.5pt,partopsep=1pt,parsep=1pt]
  \item Noisy backgrounds (where in fact caf\'e noise is different from car noise),
  \item Accented speech, for which there are particular ways to make the recognition system more robust,
  \item Far from microphone (known as far-field speech recognition),
  \item Young children's speech (how they pronounce words, and which words are used), or
  \item Stuttering (and how to handle nonsensical phrases like ``oh'', ``ah'', ``um''). \end{itemize}
It's not obvious where to focus your efforts. Instead, build quickly then iterate: set up a dev/test set and
a metric, then build an initial machine learning system and train it. The key is to
\begin{enumerate}[topsep=2pt,itemsep=0.5pt,partopsep=1pt,parsep=1pt] \item Understand performance on dev/test, 
\item Apply corresponding bias-variance analysis, and lastly
\item Set up an error analysis \end{enumerate}
in order to prioritize next steps. By having first built a quick-and-dirty system, 
we can now \emph{localize} the problem: e.g. does the error
analysis inform us that far-field speech recognition techniques could greatly benefit our performance?

\subsubsection{Mismatched Training and Dev/Test Set} Deep learning algorithms have a huge hunger for 
labeled training data. Sometimes, teams are tempted to throw more and more training data at an algorithm
in spite of it not coming from the same distribution as their dev/test set; there are some subtleties
in how we can deal with this.

\paragraph{Training and testing on different distributions} Suppose you're building a mobile application whereby users will
upload photos taken from cell phones, and you'd like to recognize whether the photograph contains a cat. For the sake of simplicity,
suppose there are two main sources of data you can obtain: (i) the distribution of data you really care about, which tends to be
less professionally shot, less well-framed, and perhaps even blurrier because it is shot by amateur users, or (ii) crawling the web
and downloading a lot of professional, high-resolution, and well-framed cat photographs. Perhaps our mobile app doesn't yet have
a lot of users, and so we have 10,000 photographs from our app but crawling the web gets us another 200,000 images.

What we care about in the end is how well our algorithm performs on the set of images drawn from the mobile phone 
distribution, because in the end that's how our users will upload their photos. But you don't \emph{just} want to train
on 10,000 images, because that's just perhaps too small a training set. One thing we could do, is take \emph{all} of the 210,000
images we have (drawn from different distributions) and shuffle them into train/dev/test sets. There are advantages and disadvantages to this approach. The advantage is that your train/dev/test sets all come from the same distribution. But, the (huge) disadvantage
is that our dev set is likely to contain a disproportionate amount of professional photographs relative to mobile uploads. In expectation, $\frac{200}{210} \times 100 \%$ of our development set will come from professional photographs, and only $\frac{10}{210} \times 100\%$ will come from mobile uploads. Therefore, Andrew recommends \emph{against} this option of randomly shuffling our data together.

Instead, we can still use the same number of images in the training set: we'll use \emph{all} of the data drawn from the distribution we \emph{don't} care about in addition to some of the data we \emph{do} (e.g. we'll use 200,000 professional images from the web and another 5,000 from our mobile phone uploads) and then the dev/test shall consist \emph{only} of images that we actually care about (e.g. the remaining 5,000 mobile phone uploads). The advantage of this method is we're now being more precise in how we specify
our dev/test set performance and how it relates to real-world performance. The disadvantage is that the distribution of our
\emph{training} data is different from that of dev/test. However, this split of data yields better performance in the long term.

Let's consider a second example. Suppose we're building a speech recognition system which replaces a rear-view mirror in a car into
a smart-device. Perhaps we've worked on speech recognition before, and we have a lot of data from other speech recognition applications but not from a \emph{speech-activated rear-view mirror}. Our training set shall consist of all speech recognition data we've
accumulated over the years (which comes in $(x,y)$ pairs where $x$ is an audio-clip and $y$ is a transcript), perhaps this includes
smart voice activated speakers, keyboards, etc. But crucially, \emph{our} rear-view mirror application will involve a lot more
\emph{navigation} requests because the device is situated in a car. Therefore, 
the distribution of our dev/test sets shall be drawn from speech activated rear-view mirrors, and will be different from our 
training data which will consist of all other data.\footnote{Perhaps if we don't think we need \emph{all} the speech-activated rear-view mirror examples in our dev/test sets (i.e. they are already sufficiently large), then we can place some examples in our training set.}

\paragraph{Bias and variance with mismatched data distributions} Estimating the bias and variance of your learning algorithm
can greatly inform what to prioritize next. But the way you analyze bias and variance changes when your training set is drawn from a different distribution than your dev/test set. Suppose we're in the realm of cat classification for images, and so Bayes optimal error is nearly zero. To carry out bias-variance analysis, we typically examine training error in conjunction with error on the dev set. Suppose training error is 1\% and development set error is 10\%. \emph{If} your training and development sets are drawn from the \emph{same} distribution, then you have a variance problem and that our algorithm is not generalizing well from the training set. However, if the data are drawn from different distributions this is no loner a safe conclusion: perhaps the development set is just much harder to classify on. The problem with this analysis was that as we changed from looking at training error to dev error, two things changed at one time: (i) the algorithm saw data in the training set but not the dev set, and (ii) the distribution of dev data 
is different. Because we changed two things at a time, it's difficult to know if the 9\% increase in error is due to the algorithm
not having seen the examples before (part of the variance problem) \emph{or} if its just because the dev set data is different.

In order to tease out these differences, we'll define a new piece of data called the \texttt{training-dev} set. This will be a new
subset of data that we carve out which will have the \emph{same} distribution as our training data \emph{but} we will \emph{not}
explicitly train our network on this data. So, to be explicit: the setting is that our dev/test sets are drawn from the same distribution but our training set is drawn from a different distribution. We'll take a partition our training data with a smaller piece 
going to the \texttt{training-dev} set, so now just as the dev/test sets have the same distribution so do the training data alongside the \texttt{training-dev} set. To carry out error analysis, you should now look at misclassified examples from \emph{each} of
the training set, the \texttt{training-dev} set, \emph{and} the development set.

Suppose we have 1\% training error, 9\% \texttt{training-dev} error, and 10\% dev set error. We can \emph{now} conclude that we have a large variance problem, since all that changed when moving from measuring error on training set to the \texttt{training-dev} set
was that our network hadn't yet seen the examples (but the distribution remained the same). So, our network isn't generalizing well
to examples drawn from the same distribution.

Suppose as a different example that we have a 1\% training error, a 1.5\% \texttt{training-dev} error, but when we go to the development set we get a 10\% error. Now, our variance problem is relatively low: the error only increased slightly in moving to unseen examples drawn from the same distribution. What we have here is a data mismatch problem.

Suppose as another example the training error is 10\%, \texttt{training-dev} is 11\%, and development set error is 12\%. Here, our largest problem is \emph{avoidable bias}, since our proxy for Bayes optimal error is zero in the cat classifying example. As a last example, suppose training error is 10\%, \texttt{training-dev} error is 11\%, and dev error is 20\%. Here, we have two issues: the avoidable bias is high, and although the variance problem is small (difference between \texttt{training-dev} and training error is small), there is a large discrepancy between \texttt{training-dev} and development set error which indicates a data mismatch problem.  

\begin{figure}[h]
\centering
\label{fig: relationshipsbwdifferrs}
\begin{tikzpicture}   \node (hlp) at (0, 0) {\textrm{Human level performance}};
  \node[below = 0.5 cm of hlp] (train) {\textrm{Training set error}}; 
  \draw[<->] (hlp) -- (train) node[midway,right,scale=0.7] {\textrm{avoidable bias}};
  \node[below = 0.5 cm of train] (traindev) {\textrm{\texttt{Training-dev} set error}};
  \draw[<->] (train) -- (traindev) node[midway,right,scale=0.7] {\textrm{variance}};
  \node[below = 0.5 cm of traindev] (dev) {\textrm{Dev error}};
  \draw[<->] (traindev) -- (dev) node[midway,right,scale=0.7] {\textrm{data mismatch}};
  \node[below = 0.5 cm of dev] (test) {\textrm{Test error}};
  \draw[<->] (dev) -- (test) node[midway, right,scale=0.7] {\textrm{over-fitting to dev set}}; \end{tikzpicture}
\caption{The relationship between different types of data and the problems we may run into when there are measurable gaps in error between adjacent levels.}
\end{figure}

Andrew also cautions that it's possible for the dev/test error to be lower than that of our training error or our \texttt{training-dev} error. Consider the following table which generalizes the above analysis:
\begin{table}[h]
\centering
\begin{tabular}{c| c c }
 & $\substack{\textrm{General speech} \\ \textrm{recognition}}$ & $\substack{\textrm{Rearview mirror} \\ \textrm{speech data}}$ \\
\hline
Human level & ``Human level'' & \\
$\substack{\textrm{Error for} \\ \textrm{output trained on}}$ & ``Training error'' & \\
$\substack{\textrm{Error for} \\ \textrm{examples \uline{not}} \\ \textrm{trained on}}$ & ``\texttt{Training-dev} error'' & ``Dev/test error''\\ \end{tabular}
\end{table}

We've seen the relationships between filled in entries in figure \ref{fig: relationshipsbwdifferrs}. If we fill in the remaining missing entries, we can sometimes gain additional insights such as, ``if human level performance varies from one problem to another, then its indicative one problem may be \emph{harder} than another''. Or, we can understand bias-variance and data mismatch problems in different degrees.

\paragraph{Addressing data mismatch} What if your training set comes from a different distribution than your dev/test sets, and furthermore your error analysis shows that you have a data mismatch problem. What can you do? Andrew recommends carrying out a \emph{manual} error analysis and trying to understand the differences between the training set and the dev set.\footnote{We remark that to avoid overfitting to your test set, for the sake of error analysis you should only manually inspect the development set.} E.g. in
the context of the speech-activated rear-view mirror application, you might listen to examples in your development set to try
and figure out how they're different from your training set: perhaps car noise is a problem that arises, or mis-classification of
numbers involved in the parsing of street numbers. Once you've identified how the development set may be harder than the training
set, you can look into ways to making the training set more similar, or perhaps collect more data that is similar to your dev/test
sets. E.g. you could simulate noisy in-car data, or you could get more data of people speaking out numbers. 
These are rough guidelines, and not systematic recommendations.

\paragraph{Artificial data synthesis}
If the goal is to make your training data more similar to your development set, one technique you could use is artificial data synthesis. In the context of the car noise problem, this could mean taking clean (background noise-free) audio examples and super-imposing them with background car noise to generate a new example. This simple audio synthesis can create more data that is more similar to your dev/test distribution. There's one caveat: suppose you have 10k hours of clean audio but only 1 hour of car noise. If we naively re-apply this one hour of car noise to all 10k hours of clean audio, our learning algorithm may over-fit to this particular
type of background noise. You could imagine that our synthesized data only simulates a very small subset of the space of all possible audio clips with background car noise.

As another example, consider self-driving cars and the sub-problem of car detection (or drawing bounding boxes around cars within still images). Some people have thought before: can we render computer generated graphics (possibly from video games) to generate additional data? The problem is that our rendered simulation may only cover a small subset of the possible space of all cars and graphics that are required by a real application. I.e. to the human eye a video game simulation with 20 unique cars may look very realistic and as though we aren't running into repeat cars and designs, but a neural network may very quickly learn to overfit to this small subset of the space of data.

\subsection{Learning from Multiple Tasks} One of the most powerful ideas in deep learning is that you can sometimes take
knowledge learned by a neural network from one task, and apply that (part of that) knowledge to a separate task; this is called 
\href{https://en.wikipedia.org/wiki/Transfer_learning}{transfer learning}. Suppose as an example that you've trained your neural
network on image recognition, where $(x,y)$ pairs where $x$ is an image and $y$ is some object (e.g. a dog, cat, bird, etc.). Suppose we're interested in adapting this neural network to make a diagnosis in radiology.

\begin{figure}[h]
\centering
\begin{tikzpicture}[darkstyle/.style={circle,draw,fill=gray!40,minimum size=20}]
  \node (x) at (0, 0) {$x$};
  \node[draw, circle, right = 0.5cm of x] (l10) {};
  \node[draw, circle, above = .3cm of l10] (l11) {};
  \node[draw, circle, below = .3cm of l10] (l12) {};
  \node[draw, fit = (l10) (l11) (l12)] (layer1) {};
  \draw[->] (x) to (layer1);
  \node[draw, circle, right = 0.5cm of layer1] (l20) {};
  \node[draw, circle, above = .1cm of l20] (l21) {};
  \node[draw, circle, above = .1cm of l21] (l22) {};
  \node[draw, circle, above = .1cm of l22] (l23) {};
  \node[draw, circle, below = .1cm of l20] (l24) {};
  \node[draw, circle, below = .1cm of l24] (l25) {};
  \node[draw, circle, below = .1cm of l25] (l26) {};
  \node[draw, fit = (l23) (l26)] (layer2) {};
  \draw[->] (layer1) to (layer2);  
  \node[draw, circle, right = 0.5cm of layer2] (l30) {};
  \node[draw, circle, above = .1cm of l30] (l31) {};
  \node[draw, circle, above = .1cm of l31] (l32) {};
  \node[draw, circle, above = .1cm of l32] (l33) {};
  \node[draw, circle, below = .1cm of l30] (l34) {};
  \node[draw, circle, below = .1cm of l34] (l35) {};
  \node[draw, circle, below = .1cm of l35] (l36) {};
  \node[draw, fit = (l33) (l36)] (layer3) {};
  \draw[->] (layer2) to (layer3);  
  \node[draw, circle, right = 0.5cm of layer3] (l40) {};
  \node[draw, circle, above = .3cm of l40] (l41) {};
  \node[draw, circle, below = .3cm of l40] (l42) {};
  \node[draw, fit = (l40) (l41) (l42)] (layer4) {};
  \draw[->] (layer3) to (layer4);
  \node[draw, circle, right = 0.5cm of layer4] (l50) {};
  \node[draw, circle, above = .3cm of l50] (l51) {};
  \node[draw, circle, below = .3cm of l50] (l52) {};
  \node[draw, fit = (l50) (l51) (l52)] (layer5) {};
  \draw[->] (layer4) to (layer5);
  \node[draw, circle, right = 0.5cm of layer5] (fnode) {};
  \node[draw, fit = (fnode)] (fnodebox) {};
  \draw[->] (layer5) to (fnodebox);
  \node[right = 0.5cm of fnodebox] (yhat) {$\hat y$};
  \draw[->] (fnodebox) to (yhat);
  \node[right = 0.5cm of yhat] (imagerecog) {\color{orange} \textrm{image recognition}};
  \node[draw, ultra thick, orange, right = 0.435cm of layer5, cross out, scale = 1.75] {};
  \node[below = 0.75cm of fnode, draw, circle] (newnode) {};
  \node[draw, fit = (newnode)] (newnodebox) {};
  \draw[->, purple] (layer5) to (newnodebox);
  \node[right = 0.5cm of newnodebox] (newyhat) {$\hat y$};
  \draw[->, purple] (newnodebox) to (newyhat);
  \node[right = 1cm of newyhat] {\color{purple} radiology};
\end{tikzpicture}
\caption{\footnotesize To adapt a neural network, we can simply delete the last output layer of our old network and also the weights feeding into the last layer. We create a new set of randomly initialized weights just for the last layer $(W^{[L]}, b^{[L]})$ and set the last layer to output
a radiology diagnosis. After training the old network on $(X,Y)$ pairs of images and objects, we can train a new network where the $X$'s are radiology images and the $Y$'s are diagnoses.}
\end{figure}

Now, when we re-train the network on new $(X,Y)$ pairs (radiology images and diagnoses, in our example), we have several options for how exactly to update our weights. If we have a small radiology dataset, we may choose to update the weights only in the last layer
$W^{[L]}$ and $b^{[L]}$ (or maybe the last two layers) and keep the remaining parameters fixed. If you have enough radiology data, you could also retrain all of the other layers in the network as well; in this case the initial phase of training on original ($X,Y$) pairs is known as pre-training
because we are using these $(X,Y)$ pairs to pre-initialize our weights and the training on radiology data is known as \emph{fine-tuning}. In this example, we've taken what we've learned from image recognition and applied it to radiology diagnosis. Why does this work? A lot of the low level features consist of things such as detecting edges and curves. If we can learn from a very large image recognition data set how to create these features, might help our learning algorithm do better in radiology diagnosis since the network has learned to understand the structure and nature of how images look.

\paragraph{Adding new intermediary layers when transfer learning}
Let's consider another example. Suppose you've trained a speech recognition system, so now our $(X,Y)$ mapping is from audio snippets to transcripts. Suppose now that we want to build a ``wake-words'' or ``trigger words'' detection system for applications like
waking up Google Home, Amazon Alexa, etc. In order to construct a new neural network, we can again take out the last layer of our
old network, reset the weights $W^{[L]}$ and $b^{[L]}$, create a new output node, and then finally train on new $(X,Y)$ pairs. But
another thing we can do is to actually create several new layers in order to better predict labels for your wake-work prediction
problem.

\begin{figure}[h]
\centering
\begin{tikzpicture}[darkstyle/.style={circle,draw,fill=gray!40,minimum size=20}]
  \node (x) at (0, 0) {$x$};
  \node[draw, circle, right = 0.5cm of x] (l10) {};
  \node[draw, circle, above = .3cm of l10] (l11) {};
  \node[draw, circle, below = .3cm of l10] (l12) {};
  \node[draw, fit = (l10) (l11) (l12)] (layer1) {};
  \draw[->] (x) to (layer1);
  \node[draw, circle, right = 0.5cm of layer1] (l20) {};
  \node[draw, circle, above = .1cm of l20] (l21) {};
  \node[draw, circle, above = .1cm of l21] (l22) {};
  \node[draw, circle, above = .1cm of l22] (l23) {};
  \node[draw, circle, below = .1cm of l20] (l24) {};
  \node[draw, circle, below = .1cm of l24] (l25) {};
  \node[draw, circle, below = .1cm of l25] (l26) {};
  \node[draw, fit = (l23) (l26)] (layer2) {};
  \draw[->] (layer1) to (layer2);  
  \node[draw, circle, right = 0.5cm of layer2] (l30) {};
  \node[draw, circle, above = .1cm of l30] (l31) {};
  \node[draw, circle, above = .1cm of l31] (l32) {};
  \node[draw, circle, above = .1cm of l32] (l33) {};
  \node[draw, circle, below = .1cm of l30] (l34) {};
  \node[draw, circle, below = .1cm of l34] (l35) {};
  \node[draw, circle, below = .1cm of l35] (l36) {};
  \node[draw, fit = (l33) (l36)] (layer3) {};
  \draw[->] (layer2) to (layer3);  
  \node[draw, circle, right = 0.5cm of layer3] (l40) {};
  \node[draw, circle, above = .3cm of l40] (l41) {};
  \node[draw, circle, below = .3cm of l40] (l42) {};
  \node[draw, fit = (l40) (l41) (l42)] (layer4) {};
  \draw[->] (layer3) to (layer4);
  \node[draw, circle, right = 0.5cm of layer4] (l50) {};
  \node[draw, circle, above = .3cm of l50] (l51) {};
  \node[draw, circle, below = .3cm of l50] (l52) {};
  \node[draw, fit = (l50) (l51) (l52)] (layer5) {};
  \draw[->] (layer4) to (layer5);
  \node[draw, circle, right = 0.5cm of layer5] (fnode) {};
  \node[draw, fit = (fnode)] (fnodebox) {};
  \draw[->] (layer5) to (fnodebox);
  \node[right = 0.5cm of fnodebox] (yhat) {$\hat y$};
  \draw[->] (fnodebox) to (yhat);
  \node[right = 0.5cm of yhat] (imagerecog) {\color{orange} \textrm{audio detection}};
  \node[draw, ultra thick, orange, right = 0.435cm of layer5, cross out, scale = 1.75] {};
  \node[below = 1cm of fnode, draw, circle] (newnode) {};
  \node[above = 0.25cm of newnode, draw, circle] (newnodep1) {};
  \node[below = 0.25cm of newnode, draw, circle] (newnodem1) {};
  \node[draw, fit = (newnodem1) (newnodep1)] (newnodebox) {};
  \draw[->, purple] (layer5) to (newnodebox);
  \node[right = 0.5cm of newnode, draw, circle] (lastlayercentered) {};
  \node[below = 0.25cm of lastlayercentered, draw, circle] (lastlayerminus) {};
  \node[above = 0.25cm of lastlayercentered, draw, circle] (lastlayerplus) {};
  \node[draw, fit = (lastlayerminus) (lastlayerplus)] (lastlayer) {};
  \node[right = 0.5cm of lastlayercentered, draw, circle] (lastnode) {};
  \node[draw, fit = (lastnode)] (penultimate) {};
  \node[right = 0.5cm of lastnode] (newyhat) {$\hat y$};
  \draw[->, purple] (newnodebox) to (lastlayer);
  \draw[->, purple] (lastlayer) to (penultimate);
  \draw[->, purple] (penultimate) to (newyhat);
  \node[right = 0.25cm of newyhat] {\color{purple} wake-words};
\end{tikzpicture}
\caption{\footnotesize In another example, we demonstrate transfer learning with wake-words. Here, we not only modify the last output layer to 
output a prediction in wake-word space, but we also incorporate several additional intermediary layers. Depending on how much
wake-words data we have available to train on, we can choose to either only train the weights of the newly incorporated/modified layers \emph{or} we can choose to re-train the entire network.}
\end{figure}

\paragraph{When does transfer learning make sense?}
It only makes sense to apply transfer learning when the inputs for both tasks are the same. E.g. in our examples above, the inputs were images in one example and audio clips in another. Further, transfer learning makes sense when we have a lot of data for the problem you're transfer learning from, and relatively less data for the problem you're transferring to. Let's reconsider our examples. In the context of image recognition, it's not uncommon to end up with a million examples for a particular task, which is quite a lot of data to learn low-level and useful features in earlier layers of our network. On the other hand, we may only have a few hundred examples for our radiology diagnosis
problem. A lot of the knowledge that we learned from image recognition can be transferred and can greatly help radiology
diagnosis even without a lot of radiology data. In the context of our speech recognition example, 
we might have 10,000 hours of data which is quite a lot, but for trigger words perhaps we only have one hour of data. In this case,
a lot of what the network learns about what human voices sound like, what the components of speech are, and so on,
can be really helpful in building a good wake-words detector. It \emph{doesn't} make sense to use transfer learning when we have little data to pre-train our network on and relatively more data to fine-tune the network on; in these cases we need to remember what
our core task is and recognize which training examples are the most informative.

\subsubsection{Multi-task Learning} In transfer learning, we have a sequential process: we start learning how to perform task A
and then transfer that knowledge to task B. In multi-task learning, we start simultaneously and try to have one neural network do several things at the same time (with the hope that each of these tasks helps in solving the other tasks).
\begin{table}[h]
  \centering
  \begin{tabular}{l | c}     & $y^{(i)}$ \\
    \hline
    Pedestrians & 0 \\
    Cars & 1 \\
    Stop-signs & 1 \\
    Traffic-lights & 0     \end{tabular}
\caption{\footnotesize In multi-task learning, our input is an image $x^{(i)}$ and our output
  $y^{(i)}$ may have one entry for each task we are interested in. Suppose you're building a self-driving car, which needs to be able to detect several types of objects like pedestrians, other cars, stop signs, traffic lights,
and maybe even other things.
 If we have an image of a car at a stop-sign, then we would
  expect the following output vector to be associated with the example.} 
\end{table} 

\begin{figure}[h]
\centering
\begin{tikzpicture}[darkstyle/.style={circle,draw,fill=gray!40,minimum size=20}]
  \node (x) at (0, 0) {$x$};
  \node[draw, circle, right = 0.5cm of x] (l10) {};
  \node[draw, circle, above = .3cm of l10] (l11) {};
  \node[draw, circle, below = .3cm of l10] (l12) {};
  \node[draw, fit = (l10) (l11) (l12)] (layer1) {};
  \draw[->] (x) to (layer1);
  \node[draw, circle, right = 0.5cm of layer1] (l20) {};
  \node[draw, circle, above = .1cm of l20] (l21) {};
  \node[draw, circle, above = .1cm of l21] (l22) {};
  \node[draw, circle, above = .1cm of l22] (l23) {};
  \node[draw, circle, below = .1cm of l20] (l24) {};
  \node[draw, circle, below = .1cm of l24] (l25) {};
  \node[draw, circle, below = .1cm of l25] (l26) {};
  \node[draw, fit = (l23) (l26)] (layer2) {};
  \draw[->] (layer1) to (layer2);  
  \node[draw, circle, right = 0.5cm of layer2] (l30) {};
  \node[draw, circle, above = .1cm of l30] (l31) {};
  \node[draw, circle, above = .1cm of l31] (l32) {};
  \node[draw, circle, above = .1cm of l32] (l33) {};
  \node[draw, circle, below = .1cm of l30] (l34) {};
  \node[draw, circle, below = .1cm of l34] (l35) {};
  \node[draw, circle, below = .1cm of l35] (l36) {};
  \node[draw, fit = (l33) (l36)] (layer3) {};
  \draw[->] (layer2) to (layer3);  
  \node[draw, circle, right = 0.5cm of layer3] (l40) {};
  \node[draw, circle, above = .3cm of l40] (l41) {};
  \node[draw, circle, below = .3cm of l40] (l42) {};
  \node[draw, fit = (l40) (l41) (l42)] (layer4) {};
  \draw[->] (layer3) to (layer4);
  \node[draw, circle, right = 0.5cm of layer4] (l50) {};
  \node[draw, circle, above = .3cm of l50] (l51) {};
  \node[draw, circle, below = .3cm of l50] (l52) {};
  \node[draw, fit = (l50) (l51) (l52)] (layer5) {};
  \draw[->] (layer4) to (layer5);
  \node[draw, circle] (fnode) at (6.25, 0.25) {};
  \node[draw, circle, above = 0.1cm of fnode] (fnodep) {};
  \node[draw, circle, below = 0.1cm of fnode] (fnodeb) {};
  \node[draw, circle, below = 0.1cm of fnodeb] (fnodebb) {};
  \node[draw, fit = (fnodebb) (fnodep)] (fnodebox) {};
  \draw[->] (layer5) to (fnodebox);
  \node[right = 0.5cm of fnodebox] (yhat) {$\hat y$};
  \draw[->] (fnodebox) to (yhat);
\end{tikzpicture}
\caption{\footnotesize Note that here our last layer has as many nodes as there are tasks we trying to solve, and correspondingly
the dimension of $\hat y$ will be the same as well.}
\end{figure}

\paragraph{Loss for a multi-task problem} Let $|y|$ denote the dimension of our output vector. We could define a cost function to be
\begin{equation*}   \sum_{i=1}^m \sum_{j=1}^{|y|} \mathcal L(\hat y_j^{(i)}, y_j^{(i)}) \end{equation*}
where $\mathcal L(\cdot, \cdot)$ is our usual Logistic loss function for a binary prediction problem. What's the difference between this and using softmax regression? In softmax regression, we assign a single label to each (training) example, whereas in multi-task
learning each example can have multiple labels. Note that we could have trained $|y|$ separate neural networks to each solve one task. But, if some of the earlier features in the neural network can be shared between different tasks, then you might find that training a single neural network to solve multiple tasks results in better performance.

\paragraph{Multi-task learning works even if inputs aren't ``fully'' labeled} Suppose we are still in the setting of self-driving cars and we have an image of a car at a stop-sign, but that our human-labeler didn't bother to mark the image as having both
a car and a stop-sign (just one of them). Maybe we're in the setting where some of our examples \emph{are} fully labeled, but others
are not. In this setting, it's still valid to perform multi-task learning. In particular, we modify our loss-function to only sum over the indices of $j$ which correspond to having been labeled. I.e.
\begin{equation*}   
  \sum_{i=1}^m \sum_{j=1}^{|y|} \mathcal L(\hat y_j^{(i)}, y_j^{(i)}) \mathbbm 1_{\{\textrm{Category } j \textrm{ was labeled on example } i\}} 
\end{equation*}

\paragraph{When does multi-task learning make sense?} It makes sense in general when three things are true.
\begin{enumerate}   \item  If the set of tasks could benefit from having shared low-level features. E.g. in the context of autonomous driving, being able to recognize traffic lights, cars, and pedestrians may help us learn features that also distinguish stop signs.
  \item If the amount of data available for each task is similar. As a soft rule, we expect a single task to benefit from multi-task
    learning if the amount of data available from all other tasks is at least as large as the amount of data we have at hand
    for one task; the reasons why are similar to when transfer learning does and doesn't work.
  \item Can train a big enough network to do well on all tasks. From researcher Rich Carona, the only times 
    that multi-task learning hurts performance compared to training separate neural networks is if your 
    neural network isn't big enough. If you can train a large enough network, multi-task learning shouldn't hurt performance. \end{enumerate}

\subsection{End-to-End Deep Learning} One of the most exciting recent developments in deep learning has been the rise
of end-to-end deep learning. What does this mean? Briefly, it means learning systems that require multiple stages of processing
and replacing it by a single neural network. Consider an example of speech recognition, where the goal is to take an input $x$
such as an audio clip and map it to a transcript $y$. Traditional speech recognition methods require many stages of processing,
e.g. \href{https://en.wikipedia.org/wiki/Mel-frequency_cepstrum}{MFCC} which is an algorithm for extracting hand designed features
of audio. Having extracted these low level features, you might apply a machine learning algorithm to find the \href{https://en.wikipedia.org/wiki/Phoneme}{phonemes} in the audio clip, the basic units of sound; e.g. the word ``cat'' consists of three sounds: ``cu'', ``ah'', and ``tu''. After phonemes are extracted, we can string them together to form words, and finally stringing together words
we get a transcript.

\begin{figure}[h]   \centering
  \begin{tikzpicture}     \node (x) at (0.15,0) {$x$};
    \node[below=0.15cm of x] (a) {audio};
    \node[right=0.5cm of a] (f) {features};
    \node[right=0.5cm of f] (p) {phonemes};
    \node[right=0.5cm of p] (w) {words};
    \node[right=0.5cm of w] (t) {transcript};
    \draw[->] (a) -- (f) node[midway,above,scale=0.7] {\textrm{MFCC}};
    \draw[->] (f) -- (p) node[midway,above,scale=0.7] {\textrm{ML}};     \draw[->] (p) -- (w) node[midway,above,scale=0.7] {``cat''}; 
    \draw[->] (w) -- (t) node[midway,above,scale=0.7] {}; 
  \end{tikzpicture}
  \caption{\footnotesize A classical pipeline for processing audio into transcripts. Such a sequential system of algorithms is no longer necessary with end-to-end deep learning: a neural network can accept as input an audio clip and return as output a transcript. Andrew admits that end-to-end deep learning systems have made obsolete many sub-fields which used to exist and which were predicated on delicate feature engineering for their particular sub-task.} \end{figure}

One downside of end-to-end learning is that you might need a lot of data before it starts to work well; for smaller sized data sets,
the hand-engineered approaches can be quite competitive. If you have a medium amount of data, there are intermediary approaches
where perhaps in our example you input the audio and bypass the features and just learn to output phonemes, and then there would be some other steps involved as well: this would be a step towards end-to-end learning.

Let's consider an interesting example of face recognition at turnstiles in public transit systems: if the system recognizes the face
it shall allow the person to pass without needing to scan an RFID card. The input here is an image which contains a person approaching the turnstile, and we could try to learn a mapping from this raw image $X$ to the identity of the person $Y$, but this turns out to \emph{not} be the best approach: the person can approach the turnstile from different directions and they can be at varying distances from the camera whereby their face appears much larger. The best approach to date is a multi-step approach where first one piece of software is run to detect \emph{where} the person's face is within the image, and as a secondary step the image is cropped 
and centered to a new neural network that tries to learn the person's \emph{identity}. 
Researchers have found that breaking the problem into two stages has resulted in the best performance.\footnote{The way the second neural network works is that it takes as input two images, and it returns whether the images contain the same person or not. In our application, the background database would consist of facial images for all employees on file: the algorithm would then scan through
all the background images and see whether the input image contains a face that matches a face contained in the database.}
Why does the two step approach work better? There are two reasons.
\begin{itemize}   
  \item Each subproblem is much simpler.
  \item Each sub-task is amenable to producing a lot of data. 
\end{itemize}
In particular, there's a lot of data one could obtain that maps input images $X$ to bounded boxes $y$ describing where a persons face in the image lie. Separately, there's a lot of data for looking at two images and determining whether its the same person. But in contrast, if you try to learn everything at the same time, there is much less data of the form $(X,Y)$ where $X$ is an input image from a turnstile, and $Y$ is the identity of the person. One example where end-to-end deep learning does work well is in machine translation, since for any arbitrary pair of countries we have a lot of mappings or translations.

\paragraph{Whether to use End-to-End deep learning}

\begin{itemize}   \item Pros
    \begin{itemize}       \item End-to-end learning lets the data speak: no matter what $(X,Y)$ mapping truly exists,
        with enough data and a big enough network, we should be able to learn the function mapping
        from $X$ to $Y$. Further, a pure machine learning approach allows the neural network to capture statistics
        of the data rather than being forced to reflect human preconceptions.\footnote{Consider an example of speech recognition. 
        Andrew believes that phonemes are an artifact created by linguists that are reasonable to the description of language. 
        However, it's not obvious that you want to force your learning algorithm to think in phonemes.}
    \item There is less hand designing of components needed, which means less time spent hand-engineering features or
      designing intermediate representations.     \end{itemize}
  \item Cons
    \begin{itemize}     \item May require a large amount of data to learn an end-to-end mapping from raw $X$ to $Y$. 
      We've seen previously that it can be easier to acquire
      data for sub-tasks, e.g. first finding a face within an image then determining if the face matches
      one in a database.
    \item It excludes potentially useful hand-designed components. If you don't have a lot of data, then
      your learning algorithm won't yield that much insight, and so hand designing a component can
      be a way to inject manual knowledge into an algorithm (which is not always a bad thing).     \end{itemize} \end{itemize} 

A learning algorithm has two main sources of knowledge: (i) the data, and (ii) whatever you hand design. If you have tons of data,
it's less important to hand-design things, but without much data hand-design becomes more important. They key question to answer when determining whether to use an end-to-end approach is: do I have sufficient data to learn the function of the complexity needed
to map $X$ to $Y$.

As an example of the boundary between end-to-end learning, consider self-driving cars. If we don't take an end-to-end approach,
we could take as input an image of what's in front of the car (perhaps obtained through picture, radar, lidar, or other sensor readings) and (i) try to figure out what objects are in the image, (ii) how to navigate the car safely around the pedestrians and cars spotted, and (iii) execute the plan with appropriate steering as well as acceleration and braking. Step two is usually not performed by a neural network, but instead by a piece of software called \href{https://en.wikipedia.org/wiki/Motion_planning}{Motion Planning}. As of this writing, Andrew believes that end-to-end learning is less effective than more sophisticated hand-engineered approaches.

\section{Convolutional Neural Networks} Computer vision is an area with a rapidly growing research body. E.g.,
computer vision can
\begin{itemize} \item Help self-driving cars figure out where the other cars and pedestrians are in order to avoid them.
\item Allow an individual to unlock a phone or door with just their face.
\item Show you the most attractive, beautiful, or relevant pictures. \end{itemize}

Deep learning is even helping new types of art to be created (see \href{https://en.wikipedia.org/wiki/Neural_Style_Transfer}{Neural Style Transfer}). Rapid advances in computer vision enable new applications
that were impossible just a few years ago. Lastly, computer vision researchers are quite cunning and there is a lot of
inspiration to be drawn from this sub-field. Let's start by reviewing some examples of computer vision.
\begin{itemize}   \item Image classification: you're given a 64x64 pixel image and asked to output a single bit describing 
    whether it contains a cat.
  \item Object Detection: you're given an image and you must determine what objects are in the picture but also draw bounding boxes around them. \end{itemize}

\paragraph{Size of input data $\leadsto$ Convolution Operation} 
If we work with a 64x64 image, and since we have three color channels RGB, each image or input is
$64^2 \cdot 3 = 12,288$ dimensional. But, this was a tiny image, what if we had a $1,000 x 1,000$ pixel image (one megapixel)? Then
each input has $1e3^2 \cdot 3 = 3e6 = 3,000,000$ or three million dimensions. What if we have 1,000 hidden units in our first layer?
Then $W^{[1]}$ will be of size $1k x 3M$ which is 3 billion parameters; with that many parameters it's difficult to get enough
data to prevent overfitting, and further the computational (and memory) requirements to train a neural network with three billion
parameters is not yet feasible. All of this is to say that if you want to use large images, you need to better implement the
convolution operation.

\subsection{Edge detection example} Earlier layers of a neural network might detect edges, later layers may detect parts
of objects, and later layers still may detect complete objects such as a person's face. In this section, we'll focus on edge 
detection. 
\subsubsection{Filters and kernels}
Suppose that we have a simple $6 \times 6$ matrix that represents a grey-scale image (we don't need the three RGB channels).
\begin{equation*}   \begin{bmatrix}     3 & 0 & 1 & 2 & 7 & 4 \\
    1 & 5 & 8 & 9 & 3 & 1 \\     2 & 7 & 2 & 5 & 1 & 3 \\     
    0 & 1 & 3 & 1 & 7 & 8 \\     
    4 & 2 & 1 & 6 & 2 & 8 \\     
    2 & 4 & 5 & 2 & 3 & 9 \\ 
  \end{bmatrix} \end{equation*}

How can we extract vertical edges? We start by constructing a $3 \times 3$ \emph{filter} matrix (also sometimes referred to as a kernel).
\begin{equation*}   \begin{bmatrix}     1 & 0 & -1 \\     1 & 0 & -1 \\
    1 & 0 & -1 \\
  \end{bmatrix} \end{equation*}

Now, what we'll do is take our original image and \emph{convolve} it with the filter; the symbol used to denote convolution is $*$,
which is distinguished from multiplication by context. The output of convolving our $6 \times 6$ image is a $4 \times 4$ matrix. Let's consider how to apply the convolution to our input image to yield the first element in the output; to do this we'll examine the first few rows and columns of our input:

\begin{equation*}   
  \begin{bmatrix}     
    3 & 0 & 1 \\
    1 & 5 & 8 \\     
    2 & 7 & 2 \\     
  \end{bmatrix} 
  *
  \begin{bmatrix}     
    1 & 0 & -1 \\     
    1 & 0 & -1 \\
    1 & 0 & -1 \\
  \end{bmatrix}
  = 3\cdot 1 + 1 \cdot 1 + 2\cdot 1 + 0\cdot 0 + 5 \cdot 0 + 7 \cdot 0 + 1 \cdot -1 + 8 \cdot -1 + 2 \cdot -1 = -5.
\end{equation*}
Notice that this is a simple element-wise product followed by a summation. To continue the process, we move the filter
over by one column and repeat the same process (element-wise multiplication followed by summation) 
to get the output for the second item in the first row. Continuing on, we get

\begin{equation*}   
  \begin{bmatrix}     
    3 & 0 & 1 & 2 & 7 & 4 \\
    1 & 5 & 8 & 9 & 3 & 1 \\     
    2 & 7 & 2 & 5 & 1 & 3 \\     
    0 & 1 & 3 & 1 & 7 & 8 \\     
    4 & 2 & 1 & 6 & 2 & 8 \\     
    2 & 4 & 5 & 2 & 3 & 9 \\ 
  \end{bmatrix} 
  *
  \begin{bmatrix}     
    1 & 0 & -1 \\     
    1 & 0 & -1 \\
    1 & 0 & -1 \\
  \end{bmatrix}
  = \begin{bmatrix}     -5 & -4 & 0 & 8 \\
    -10 & -2 & 2 & 3 \\
    0 & -2 & -4 & -7 \\
    -3 & -2 & -3 & -16   \end{bmatrix}.
\end{equation*}

This turns out to detect (vertical) edges. Why? Let's first consider a simpified example
\begin{equation*}   \begin{bmatrix}
 10 & 10 & 10 & 0 & 0 & 0 \\
 10 & 10 & 10 & 0 & 0 & 0 \\
 10 & 10 & 10 & 0 & 0 & 0 \\
 10 & 10 & 10 & 0 & 0 & 0 \\
 10 & 10 & 10 & 0 & 0 & 0 \\
 10 & 10 & 10 & 0 & 0 & 0
  \end{bmatrix} \end{equation*}
Mathematically,
\begin{equation*}   \begin{bmatrix}
 10 & 10 & 10 & 0 & 0 & 0 \\
 10 & 10 & 10 & 0 & 0 & 0 \\
 10 & 10 & 10 & 0 & 0 & 0 \\
 10 & 10 & 10 & 0 & 0 & 0 \\
 10 & 10 & 10 & 0 & 0 & 0 \\
 10 & 10 & 10 & 0 & 0 & 0
  \end{bmatrix} 
  * 
  \begin{bmatrix}     
    1 & 0 & -1 \\     
    1 & 0 & -1 \\
    1 & 0 & -1 \\
  \end{bmatrix}
  =
  \begin{bmatrix}     0 & 30 & 30 & 0 \\
    0 & 30 & 30 & 0 \\
    0 & 30 & 30 & 0 \\
    0 & 30 & 30 & 0 \\
  \end{bmatrix} \end{equation*}
If we were to visualize this image, it might look as follows:
\begin{figure}[h]   \centering
  \begin{tikzpicture}[scale=0.5]     \draw[fill = white, black] (1,0) rectangle (2, 2);     \draw[fill = white] (0,0) rectangle (1, 2); 
  \end{tikzpicture} \end{figure} The larger values yield brighter pixel intensive values. In this image, there is very clearly a vertical edge right down the middle.
We can visualize our filter or kernel matrix as well.
\begin{figure}[h]
  \centering
  \begin{tikzpicture}[scale=0.5]     \draw[fill = white] (0,0) rectangle (1, 2); 
    \draw[fill = white, black] (1,0) rectangle (2, 2);     
    \node at (3, 1) {*};
    \draw[fill = white] (4,0) rectangle (4.6666, 2); 
    \draw[fill = gray]  (4.6666,0) rectangle (5.3333, 2);
    \draw[fill = black] (5.3333,0) rectangle (6, 2);     
    \node at (7, 1) {=};
    \draw[fill = black] (8,0) rectangle (8.6666, 2); 
    \draw[fill = white]  (8.6666,0) rectangle (9.3333, 2);
    \draw[fill = black] (9.3333,0) rectangle (10, 2);     
  \end{tikzpicture} \end{figure}

Two remarks: (i) since our image is small ($64 \times 64$), the vertical edge detected appears ``quite wide'', but this is simply
an artifact of having a small-resolution image, and (ii) the enhanced pixel intensity in the middle of the output is a way of
saying that there exists a strong vertical edge in this part of the original image.

\paragraph{Edge Detection: a second example} 
We define a positive edge as one where the light goes from light to dark, versus a negative edge
which goes from a dark to a light transition. We will learn about other edge detectors, and also have a chance to understand
how an algorithm can learn to construct a filter (rather than creating one by hand).

Note that the filter we used above does take into account positive versus negative edges. If our input image was flipped
then the signs of the entries would inform us whether the edge is positive or negative.
\begin{equation*}   \begin{bmatrix}
 0 & 0 & 0 & 10 & 10 & 10 \\
 0 & 0 & 0 & 10 & 10 & 10 \\
 0 & 0 & 0 & 10 & 10 & 10 \\
 0 & 0 & 0 & 10 & 10 & 10 \\
 0 & 0 & 0 & 10 & 10 & 10 \\
 0 & 0 & 0 & 10 & 10 & 10
  \end{bmatrix} 
  * 
  \begin{bmatrix}     
    1 & 0 & -1 \\     
    1 & 0 & -1 \\
    1 & 0 & -1 \\
  \end{bmatrix}
  =
  \begin{bmatrix}     0 & -30 & -30 & 0 \\
    0 & -30 & -30 & 0 \\
    0 & -30 & -30 & 0 \\
    0 & -30 & -30 & 0 \\
  \end{bmatrix} 
\end{equation*}

As a picture, this might look like:
\begin{figure}[h]
  \centering
  \begin{tikzpicture}[scale=0.5]
    \draw[fill = gray] (0,0) rectangle (1, 2); 
    \draw[fill = white] (1,0) rectangle (2, 2);     
    \node at (3, 1) {*};
    \draw[fill = white] (4,0) rectangle (4.6666, 2); 
    \draw[fill = gray]  (4.6666,0) rectangle (5.3333, 2);
    \draw[fill = black] (5.3333,0) rectangle (6, 2);     
    \node at (7, 1) {=};
    \draw[fill = gray] (8,0) rectangle (8.6666, 2); 
    \draw[fill = black]  (8.6666,0) rectangle (9.3333, 2);
    \draw[fill = gray] (9.3333,0) rectangle (10, 2);     
\end{tikzpicture} 
\end{figure}

Since the shade of transitions is now reversed, so are the signs on the values
in our matrix. If you don't care whether the edge is positive or negative,
it's reasonable to take absolute values of the output.

\paragraph{Horizontal Edges} It may not be surprising to learn that the filter
matrix used to detect horizontal edges is simply a transpose of the filter matrix
used to detect vertical edges. I.e.
\begin{equation*} \begin{bmatrix}   1 & 1 & 1 \\ 0 & 0 & 0 \\ -1 & -1 & -1 \end{bmatrix}   \end{equation*}
Let's consider a more interesting example. The matrix
\begin{equation}   \begin{bmatrix}     10 & 10 & 10 & 0 & 0 & 0 \\
    10 & 10 & 10 & 0 & 0 & 0 \\
    10 & 10 & 10 & 0 & 0 & 0 \\
    0 & 0 & 0 & 10 & 10 & 10 \\
    0 & 0 & 0 & 10 & 10 & 10 \\
    0 & 0 & 0 & 10 & 10 & 10 \\   \end{bmatrix} \end{equation}
As applied to our filter:
\begin{equation*}   \begin{bmatrix}     
    10 & 10 & 10 & 0 & 0 & 0 \\
    10 & 10 & 10 & 0 & 0 & 0 \\
    10 & 10 & 10 & 0 & 0 & 0 \\
    0 & 0 & 0 & 10 & 10 & 10 \\
    0 & 0 & 0 & 10 & 10 & 10 \\
    0 & 0 & 0 & 10 & 10 & 10 \\       \end{bmatrix}
  *
  \begin{bmatrix}   1 & 1 & 1 \\ 0 & 0 & 0 \\ -1 & -1 & -1 \end{bmatrix}.
  =
  \begin{bmatrix}     0 & 0 & 0 & 0 \\
    30 & 10 & -10 & 30 \\
    30 & 10 & -10 & 30 \\
    0 & 0 & 0 & 0   \end{bmatrix} \end{equation*}
Graphically, our input image is ``lit-up'' in the upper left and lower right hand corners.
\begin{figure}[h]   \centering
  \begin{tikzpicture}     \draw[fill=gray] (0,0) rectangle (1,1);
    \draw (0,1) rectangle (1,2);      \draw[fill=gray] (1,1) rectangle (2,2);
    \draw (1,0) rectangle (2,1);
  \end{tikzpicture} \end{figure}
Andrew comments that the fact we see intermediate values in the middle columns is an artifact of using a low-resolution
input image. In the cases where the filter outputs $\pm 10$ in our example above, this reflects the fact that the filter
captures positive edges on one side and negative edges on the other, and so blending these together gives an intermediary value.

There is another filter matrix we could use, called a Sobel filter. It's shaped like
\begin{equation*}   
  \begin{bmatrix}     1 & 2 & 1 \\ 0 & 0 & 0 \\ -1 & -2 & -1     \end{bmatrix}
\end{equation*}

The advantage of this filter is that it places more weight on the central pixel, and this can make
the filter a bit more robust. Computer vision researchers will try other weights as well. E.g. the Scharr filter,

\begin{equation*}   
  \begin{bmatrix}     3 & 10 & 3 \\ 0 & 0 & 0 \\ -3 & -10 & -3     \end{bmatrix}
\end{equation*}
This filter has other properties. What we've shown above is just for \emph{vertical} edge detection,
but we can also transpose each of these filters to get a \emph{horizontal} edge detector.
With the rise of deep learning, perhaps we don't need to pick these numbers by hand: we can instead treat these
as parameters and have the neural network learn weights through back propagation, where the goal is to learn nine
parameters so that when you take the image and convolve it with the filter, we yield a ``good'' edge detector.
If the network wants, it can learn the same values as what are in a Sobel or Scharr filter, but its more likely they
will learn something else that is even more effective at detecting edges. Further, perhaps we can learn filters which
detect edges at arbitrary angles (e.g. an edge at 45 degrees). Underlying all these computations is the convolution,
which allows back propagation to learn whatever filter it wants.

\paragraph{Padding} One modification we can make to the basic convolutional operation is padding. In general, if we have
an $n \times n$ matrix with an $f \times f$ size filter, the output will be of size $(n-f+1) \times (n-f+1)$. There are two downsides
here:
\begin{enumerate} \item Every time we apply a convolution operator, our image shrinks. Perhaps we don't want our image to shrink each time
  we detect edges or other features; if we have many layers in our network then the latter layers take as input
  a shrunken image which could end up being a $1 \times 1$ pixel in the end.
\item If we examine corner pixels, we notice that the filter only \emph{touches} the pixel once, whereas for a pixel
in the middle of the image there are many $f \times f$ regions that overlap with the pixel. And so, it's as if pixels on
the corners of the image are used much less in the output; it's as though we're throwing away a lot of information
for these corner pixels. \end{enumerate}

\begin{figure}[h]   \centering
  \begin{tikzpicture}[scale=0.5]     \draw[step=1.0, very thick] (0,0) grid (5,5);
    \filldraw[pattern=north west lines,pattern color=orange] (0,4) rectangle(1,5);
    \draw[step=1.0, thick, orange] (0,2) grid (3,5);
    \draw[step=1.0, very thick] (6,0) grid (11, 5);
    \filldraw[pattern=north east lines, pattern color = purple] (8,2) rectangle (9,3);
    \draw[step=1.0, very thick, purple] (8,3) rectangle (11,0);
    \draw[step=1.0, very thick, purple] (6,5) rectangle (9,2);
  \end{tikzpicture}
  \caption{\footnotesize On the left, we visualize how the upper-left pixel can only be touched once by a filter. On the right,
    we highlight a central cell and visualize how it can be touched by more than one filter.} \end{figure}

To fix both of these problems, we can \emph{pad} our input image.
\begin{figure}   \centering
  \begin{tikzpicture}     \draw[step=1.0, very thick, orange] (-1,-1) grid (6,6);
    \draw[step=1.0, very thick]         (0, 0) grid (5,5);
    \filldraw[pattern=north west lines,pattern color=purple] (0,4) rectangle(1,5);
    \draw[->] (7.5, 3) -- (8.5, 3);
    \draw[step=1.0, very thick]         (10, 0) grid (15,5);
    \filldraw[pattern = north west lines, pattern color = purple] (10,5) rectangle (12,3);   \end{tikzpicture}
  \caption{\footnotesize If we pad our input image with two additional rows and two additional columns (depicted on the left), 
    we preserve the size of the image
    in the output. Further, corner cells are able to \emph{influence} the more than just one value in the output image 
    (depicted on the right).} \end{figure}

Now, if we take our \emph{padded} image and apply a convolution operation, we manage
to preserve the size of the input image. I.e. the convolution outputs an image of the same size
as the input. By convention, we pad with zeros. In general, if we pad with $p$ additional cells
on each side of the image, then our input image has dimension $(n+2p) \times (n+2p)$, and if we apply a convolution
operation we end up with a $(n+2p-f+1) \times (n+2p-f+1)$ output image.

\end{document}

