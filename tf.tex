\documentclass[12pt]{article}  

\usepackage[boxruled,lined]{algorithm2e}
%% \usepackage{booktabs}
\usepackage{amsmath} 
\usepackage{amsthm} 
\usepackage{amsfonts} 
\usepackage{enumitem}
\usepackage[T1]{fontenc}
\usepackage{xparse} 
\usepackage{bm}
\usepackage{bbm} 
\usepackage{color,soul} 
\usepackage{framed}
\usepackage[margin=0.5in]{geometry}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{mathtools}
\usepackage{multicol}
\usepackage[dvipsnames]{xcolor}
\usepackage{tikz}
\usepackage[normalem]{ulem}
\usepackage{pgfplots}  
\usepackage{pifont}
\usetikzlibrary{positioning}
\usetikzlibrary{calc}
\usetikzlibrary{fit}
\usetikzlibrary{shapes}
\usetikzlibrary{patterns}
\usetikzlibrary{decorations.pathreplacing}
\newcommand\STAR{{\tikz{\node[draw,star,star point height=.7em,minimum size=1em,scale=0.35]{};} }}
\newcommand{\Plus}{\mathord{\begin{tikzpicture}[baseline=0ex, line width=1, scale=0.13]
\draw (1,0) -- (1,2); \draw (0,1) -- (2,1); \end{tikzpicture}}}

\lstdefinelanguage{JavaScript}{
  keywords={async, await, break, case, catch, const, continue, debugger, default, delete, do, else, finally, for, function, if, in, instanceof, new, return, switch, this, throw, try, typeof, var, void, while, with},
  morecomment=[l]{//},
  morecomment=[s]{/*}{*/},
  morestring=[b]',
  morestring=[b]",
  sensitive=true
}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\D}{\mathrm{d}}
\SetKwInput{KwInput}{Input}
\SetKwInput{KwOutput}{Output}

\begin{document}
\renewcommand{\d}[1]{\ensuremath{\operatorname{d}\!{#1}}}

\tableofcontents
\newpage
\section{Browser Based Models with TensorFlow.js}\vspace{.1pt} \hrule height 2pt \smallskip \renewcommand{\arraystretch}{1}
\subsection{Training and Inference using Tensorflow.js in JavaScript} We typically think of training a neural network using GPU's or a large data center, but modern web browsers have come a long way: they contain fully fleshed out runtime environments. One of the exciting aspects of Tensorflow.js is that it allows us to train neural networks and perform inference directly from a browser. We'll see how, for example, a user can upload a picture or grab a snapshot from a webcam and then train a neural network and/or perform inference right in the browser, without ever needing to send that image up to the cloud to be processed by a server. This saves time as we cut down on communication costs,
allows us to run our models offline, and preserves user privacy.

\subsubsection{Getting Your System Ready} We're going to learn how to run all the examples and exercises for this course locally on a single machine. We'll use \href{https://www.google.com/chrome/}{Chrome} for a browser, \href{http://brackets.io/}{Brackets} for an HTML editor, and the \href{https://chrome.google.com/webstore/detail/web-server-for-chrome/ofhbbkphhbklhfoeikjpcbhemlocgigb?hl=en}{Web Server for Chrome App} as our web server. We will also use \href{https://github.com/lmoroney/dlaicourse}{github.com/lmoroney/dlaicourse} as a repository to store homework assignments.

\subsubsection{API Stack}
\begin{itemize}
\item At the highest level, we have the \emph{Keras Model} layers API which   we've learned how to use in the deep learning specialization.
\item Beneath this sits the \emph{Core API}, which is backed by a TensorFlow saved model. Using this Core API, we can interact with either a browser or node.js, and this is critically what allows us to use things like layers from keras.
\begin{itemize}
\item A browser sits ontop of \href{https://en.wikipedia.org/wiki/WebGL}{webgl},   a JavaScript API for rendering graphics; we implicitly have access to a GPU here!
\item \emph{Node.js} can rely on a TensorFlow CPU, GPU, or TPU.
\end{itemize}
\end{itemize}

\subsubsection{Building a First Model} Let's start by creating the simplest possible web-page.
\begin{verbatim}
<html>
<head></head>
<body>
  <hl>First HTML Page</hl>
</body>
</html>
\end{verbatim}

We'll next need to add a \emph{script tag} below the head and above the body to load the TensorFlow.js file.

\begin{verbatim}
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest"></script>
\end{verbatim}

We're going to build a model that learns the relationship between two numbers $x,y$ where their ground truth relationship is $y = 2x - 1$. We will do this in a \emph{separate script block}, that needs to be placed above the \texttt{body} tag in your HTML page.

\begin{verbatim}
<script lang="js">
    const model = tf.sequential();
    model.add(tf.layers.dense({units: 1, inputShape: [1]}));
    model.compile({loss:'meanSquaredError',
                   optimizer:'sgd'});
    model.summary();
</script>
\end{verbatim}

The first line defines a \emph{sequential} model. The second line adds a single hidden layer, itself containing a single hidden neuron. We then compile the model using mean-squared-error as our loss function, which will work well to model a linear relationship $y=2x-1$, and we choose stochastic gradient descent as our method of optimization. Note that in the model summary, we'll be told there are two parameters in the model, since we are learning both a weight \emph{and} a bias term. Before closing the script tag, let's insert some data that will be used to train the neural network.

\begin{verbatim}
const xs = tf.tensor2d([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0], [6, 1]);
const ys = tf.tensor2d([-3.0, -1.0, 2.0, 3.0, 5.0, 7.0], [6, 1]);
\end{verbatim}
Notice that we're defining this as a \texttt{tensor2d}, since we don't have something like \texttt{numpy} from Python. As its name suggests, we must specify the extents of the two dimensions via the second argument.

\subsubsection{Training the Model} Training should by \emph{asynchronous}, because it takes an indeterminate amount of time to complete. Our next piece of code will call an asynchronous function called \texttt{doTraining}, which, when it completes execution will do something. Because training can take an indeterminate amount of time, we don't want to block the browser while this is taking place, so instead we specify it as an asynchronous function that calls us back when it's done.
\begin{lstlisting}[language=JavaScript]
doTraining(model).then(() => {
    alert(model.predict(tf.tensor2d([10], [1,1])));
});
\end{lstlisting}
We call the function by passing it a model we just created above; when it calls back, the model is trained and at that point we can call \texttt{model.predict()}. In this example, we're predicted $\hat y$ for an input of $x = 10$. Note that we still must use a \texttt{tensor2d}, which in this case is just a scalar i.e. a tensor of dimension $1 \times 1$. To actually define an asynchronous training function, we can do so as follows:

\begin{lstlisting}[language=JavaScript]
  async function doTraining(model){
      const history =
            await model.fit(xs, ys,
                  { epochs: 500,
                    callbacks:{
                        onEpochEnd: async(epoch, logs) =>{
                            console.log("Epoch:" + epoch + " Loss:" + logs.loss);
                        }
                    }
                  });
  }
\end{lstlisting}

This code should be placed at the top of the script block that we've been creating. Because \texttt{doTraining} is asynchronous, we use keyword \texttt{await} to wait for the result. Note that after feeding the \texttt{xs} and \texttt{ys} as input arguments, the rest of the input is a JSON list, with each list-item denoted by a name followed by a colon, followed by a value. For callbacks, we can specify it on the fly in the list, where the callback itself is defined as a \texttt{list} and \texttt{onEpochEnd} is a function. To be clear, we're adding a function as a list-item. In our example, upon each epoch ending, we take the epoch number and \texttt{logs} as parameters so we can print out the information to console.

Because \texttt{doTraining} is asynchronous, when we call it we'll use a \texttt{then} clause to specify what should happen upon completion of execution.

\subsection{Training Models with CSV Files}
\paragraph{The \href{https://archive.ics.uci.edu/ml/datasets/iris}{Iris         Dataset}} We've covered a simple example where we stored the values of our data in memory. A more commmon scenario is when data comes in from an outside source such as a database connection or an imported dataset. One of the most common ways of getting data into an ML model, and JavaScript ones are no exception, is reading data from CSV files. TensorFlow.js provides facilities for this. We'll work with an \emph{Iris} dataset, which contains 150 samples taken from three types of Iris flower with 50 from each type. There are four features and a label for each observation.

\subsubsection{Reading the Data} How can we do this using JavaScript and TensorFlow.js. Before we code, we can examine our CSV file. Note the first line contains column headers. The features encode $\{\textrm{sepal}, \textrm{petal}\} \times \{\textrm{length}, \textrm{width}\}$. We'll start by placing an asynchronous function into a JavaScript block; it must be asynchronous because we'll be waiting for some values, e.g. while training.

\begin{lstlisting}[language=JavaScript]
  asynch function run() { 
  }
\end{lstlisting}

To load the data from a CSV, we'll use the \texttt{tf.data.csv} class to handle loading and parsing the data.
\begin{lstlisting}[language=JavaScript]
  const csvUrl = 'iris.csv';
  const trainingData = tf.data.csv(csvUrl, {
    columnConfigs: {
      species: {
        isLabel: true
      }
    }
  });
\end{lstlisting}

There are some important details to note. The CSV resides at a URL: we don't have the server or protocol details, which means its going to try and load it from the same directory as the web-page which is hosting the application. But, it's critical to note that we're not loading from the file system directly; it's going through the HTTP stack to get the file, so you'll need to run this code on a web server.\footnote{What's nice about the Brackets IDE is that it has a built-in web server.} In defining our training data, we make a call to \texttt{tf.data.csv} passing the URL; since tensorflow doesn't know anything about our features or labels, and so we instruct it which is our label via list syntax.

The data are returned from \texttt{tf.data.csv} as dictionaries, and for training we'll want to convert them into arrays. We'll also one-hot encode our string labels. To create a one-hot encoding, we can do so as follows:

\begin{lstlisting}[language=JavaScript]
  const convertedData =
    trainingData.map(( {xs, ys} => {
      const labels = [
        ys.species == "setosa" ? 1 : 0,
        ys.species == "virginica" ? 1 : 0,
        ys.species == "versicolor" ? 1 : 0 ]
        return{ xs: Object.values(xs), ys: Object.values(labels) };
      }).batch(10);
\end{lstlisting}
The values that werent flagged as labels are in the \texttt{xs} data-structure. If we call \texttt{Object.values(xs)}, we get back an array-of-arrays containing their values. Each row in the data-set had four features which yields a $4 \times 1$ array. These are then loaded into an array of length the size of the dataset (in this case 150 examples). Notice that we're also calling \texttt{Object.values(labels)}, which returns an array-of-arrays back as well (in this case each label is a $3 \times 1$ array and we'll have 150 of them). Ultimately, our function returns a set of features that we'll train on alongside one-hot encoded labels.

\subsubsection{Designing the Neural Network} We'll choose to create a neural network with the following architecture: four features map to a single hidden layer with five neurons which then map to an output layer with three nodes that we'll use for classification. This is how what that looks like in code:
\begin{lstlisting}[language=JavaScript]
  const model = tf.sequential();

  model.add(tf.layers.dens({
    input.shape: [numOfFeatures],
    activation: "sigmoid", units: 5}))

  model.add(tf.layers.dense({activation: "softmax", units: 3}));

  model.compile({
    loss: "categoricalCrossentropy",
    optimizer: tf.train.adam(0.06)})

  await model.fitDataset(
    convertedData,
    {
      epochs: 100,
      callbacks: {
        onEpochEnd: async(epoch, logs) => {
          console.log("E: " + epoch + " Loss: " + logs.loss);
        }
      }
    });
  \end{lstlisting}

Here, we're using a slightly different function: \texttt{model.fitDataset}. By creating importing our CSV file as a dataset, we've already done a lot of the pre-processing necessary to plug-and-play with this function. We pass the data in as first parameter, then pass a list of JSON style name values with things such as number of epochs or callback behaviors defined. If we want to use the model to do inference and get a prediction, we must first create an input tensor with feature values and pass it to the predict method of the model.

\begin{lstlisting}[language=JavaScript]
  const testVal = tf.tensor2d([5.8, 2.7, 5.1, 1.9], [1, 4]);
  const prediction = model.predict(testVal);
  alert(prediction);
\end{lstlisting}

Here, the \texttt{testVal} object holds the sepal and petal length and width; when we pass it to the predict method, we get a tensor back with a prediction in it.

\end{document}